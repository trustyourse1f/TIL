# 6.3 로이터 뉴스 분류하기(Reuters News Classification)

케라스에서 제공하는 로이터 뉴스 데이터를 LSTM을 이용하여 텍스트 분류를 진행해보자. 로이터 뉴스 기사 데이터는 총 11,258개의 뉴스 기사가 46개의 뉴스 카테고리로 분류되는 뉴스 기사 데이터이다.

## **1. 로이터 뉴스 데이터에 대한 이해**

```javascript
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import reuters
```

케라스 데이터셋으로부터 로이터 뉴스 기사 데이터를 로드하고, 뉴스 기사 데이터를 훈련용과 테스트용으로 나누겠다. reuters.load_data()에서 num_words는 이 데이터에서 등장 빈도 순위로 몇 번째에 해당하는 단어까지만 사용할 것인지 조절한다. 예를 들어서 100이란 값을 넣으면, 등장 빈도 순위로 상위 1~100 등에 해당하는 단어만 사용하게 된다. 모든 단어를 사용하고자 한다면 None으로 설정한다. test_split은 전체 뉴스 기사 데이터 중 테스트용 뉴스 기사로 몇 퍼센트를 사용할 것인지를 의미한다. 이번 실습에서는 전체 뉴스 기사 중 20%를 테스트용 뉴스 기사로 사용할 것이므로 0.2로 설정했다. 훈련용 뉴스 기사와 테스트용 뉴스 기사가 8:2로 정상적으로 분리되어 로드되는지 확인해보자.

```python
(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)

print('훈련용 뉴스 기사 : {}'.format(len(X_train)))
print('테스트용 뉴스 기사 : {}'.format(len(X_test)))
num_classes = len(set(y_train))
print('카테고리 : {}'.format(num_classes))
```

실행 결과:

```python
훈련용 뉴스 기사 : 8982
테스트용 뉴스 기사 : 2246
카테고리 : 46
```

훈련용 뉴스 기사는 8,982개, 테스트용 뉴스 기사는 2,246개, 카테고리는 46개인것을 확인할 수 있다. 훈련용 뉴스 기사 데이터의 구성을 확인하기 위해 첫번째 뉴스 기사를 출력해보았다.

```bash
print('첫번째 훈련용 뉴스 기사 :',X_train[0])
print('첫번째 훈련용 뉴스 기사의 레이블 :',y_train[0])
```

실행 결과:

```python
첫번째 훈련용 뉴스 기사 : [1, 27595, 28842, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]
첫번째 훈련용 뉴스 기사의 레이블 : 3
```

위와 같이 훈련용 뉴스 기사 데이터인 X_train 중 첫번째 뉴스 기사인 X_train[0]에는 정수의 나열이 저장되어있다. 텍스트가 아니라서 의아할 수 있는데, 현재 이 데이터는 토큰화과 정수 인코딩(각 단어를 정수로 변환)이 끝난 상태이다.

이 데이터는 단어들이 몇 번 등장하는 지의 빈도수에 따라서 정수를 부여했다. 1이라는 숫자는 이 단어가 이 데이터에서 등장 빈도가 1등이라는 뜻이다. 27,595라는 숫자는 이 단어가 데이터에서 27,595번째로 빈도수가 높은 단어라는 뜻이다. 즉, 실제로는 빈도가 굉장히 낮은 단어이다. 앞서 num_words에다가 None을 부여했는데, 만약 num_words에 1,000을 넣었다면 빈도수 순위가 1,000 이하의 단어만 가져온다는 의미이므로 데이터에서 1,000을 넘는 정수는 나오지 않는다.

뉴스 기사들의 레이블들을 의미하는 y_train에서 첫번째 뉴스 기사의 레이블인 y_train[0]에는 3이라는 값이 들어있다. 이 숫자는 첫번째 훈련용 뉴스 기사가 46개의 카테고리 중 3에 해당하는 카테고리임을 의미한다. 방금 확인한 X_train[0]과 y_train[0]은 8,982개의 훈련용 뉴스 기사 중 첫번째 뉴스 기사의 본문과 레이블만 확인한 것이다. 이번에는 8,982개의 훈련용 뉴스 기사의 길이가 대체적으로 어떤 크기를 가지는지 확인해보자.

```python
print('뉴스 기사의 최대 길이 :{}'.format(max(len(sample) for sample in X_train)))
print('뉴스 기사의 평균 길이 :{}'.format(sum(map(len, X_train))/len(X_train)))

plt.hist([len(sample) for sample in X_train], bins=50)
plt.xlabel('length of samples')
plt.ylabel('number of samples')
plt.show()
```

실행 결과:

```python
뉴스 기사의 최대 길이 : 2376
뉴스 기사의 평균 길이 : 145.5398574927633
```



![img](https://wikidocs.net/images/page/22933/reuter_histogram.PNG)

대체적으로 대부분의 뉴스가 100~200 사이의 길이를 가진다. 각 뉴스의 레이블 값의 분포를 보자.

```undefined
fig, axe = plt.subplots(ncols=1)
fig.set_size_inches(12,5)
sns.countplot(y_train)
```

![img](https://wikidocs.net/images/page/22933/%EB%B6%84%ED%8F%AC.png)

3, 4가 가장 많은 레이블을 차지하는 것을 확인할 수 있다. 각 레이블에 대한 정확한 개수를 알아보자.

```python
unique_elements, counts_elements = np.unique(y_train, return_counts=True)
print("각 레이블에 대한 빈도수:")
print(np.asarray((unique_elements, counts_elements)))
```

실행 결과:

```python
각 레이블에 대한 빈도수:
[[   0    1    2    3    4    5    6    7    8    9   10   11   12   13
    14   15   16   17   18   19   20   21   22   23   24   25   26   27
    28   29   30   31   32   33   34   35   36   37   38   39   40   41
    42   43   44   45]
 [  55  432   74 3159 1949   17   48   16  139  101  124  390   49  172
    26   20  444   39   66  549  269  100   15   41   62   92   24   15
    48   19   45   39   32   11   50   10   49   19   19   24   36   30
    13   21   12   18]]
```

3번 레이블은 총 3,159개가 존재하고 4번 레이블은 총 1,949개가 존재하는 것을 확인할 수 있다. X_train에 들어있는 숫자들이 각자 어떤 단어들을 나타내고 있는지 확인해보자. reuters.get_word_index는 각 단어와 그 단어에 부여된 인덱스를 리턴한다.

```bash
word_to_index = reuters.get_word_index()
print(word_to_index)
```

실행 결과:

```python
{'mdbl': 10996, 'fawc': 16260, 'degussa': 12089, 'woods': 8803, 'hanging': 13796, 'localized': 20672, 'sation': 20673, 'chanthaburi': 20675, 'refunding': 10997, 'hermann': 8804, 'passsengers': 20676, 'stipulate': 20677, 'heublein': 8352, 'screaming': 20713, 'tcby': 16261, 'four': 185, 'grains': 1642, 'broiler': 20680, 'wooden': 12090, 'wednesday': 1220, 'highveld': 13797, 'duffour': 7593, '0053': 20681, 'elections': 3914, '270': 2563, '271': 3551, '272': 5113, '273': 3552, '274': 3400, 'rudman': 7975, '276': 3401, '277': 3478, '278': 3632, '279': 4309, 'dormancy': 9381, - 이하 중략 -}
```

많은 단어가 출력되므로 출력 결과는 중략했다. 이번에는 정수로부터 단어를 알 수 있도록 해보자. word_to_index에서 key와 value를 반대로 저장한 index_to_word를 만든다. 주의할 점은 reuters.get_word_index()에 저장된 값에 +3을 해야 실제 맵핑되는 정수이다. 이것은 로이터 뉴스 데이터셋에서 정한 규칙이다.

```diff
index_to_word = {}
for key, value in word_to_index.items():
    index_to_word[value+3] = key
```

index_to_word[ ]에다가 인덱스를 입력하면 단어를 확인할 수 있다. +3을 했으므로 빈도수 1등에 해당하는 단어를 알고싶다면 숫자 4를 넣어야 한다. 보통 불용어로 분류되는 the가 이 데이터에서도 어김없이 등장 빈도수로 1위를 차지했다.

```lua
print('빈도수 상위 1번 단어 : {}'.format(index_to_word[4]))
```

실행 결과:

```python
빈도수 상위 1번 단어 : the
```

이번에는 임의로 빈도수 128등 단어를 알아보자.

```lua
print('빈도수 상위 128등 단어 : {}'.format(index_to_word[131]))
```

실행 결과:

```python
빈도수 상위 128등 단어 : tax
```

index_to_word에서 숫자 0은 패딩을 의미하는 토큰인 pad, 숫자 1은 문장의 시작을 의미하는 sos, 숫자 2는 OOV를 위한 토큰인 unk라는 특별 토큰에 맵핑되어져야 한다. 이를 반영하여 index_to_word를 완성해준다. 이 또한 로이터 뉴스 데이터셋에서 정한 규칙이므로 납득하고 넘어가자. index_to_word를 이용해서 첫번째 훈련용 뉴스 기사인 X_train[0]가 어떤 단어들로 구성되어있는지를 복원해보겠다. X_train[0]에 있는 모든 단어들을 하나씩 불러와서 index_to_word의 입력으로 넣고 그 결과를 연결하면 된다.

```perl
for index, token in enumerate(("<pad>", "<sos>", "<unk>")):
  index_to_word[index] = token

print(' '.join([index_to_word[index] for index in X_train[0]]))
```

실행 결과:

```python
<sos> mcgrath rentcorp said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3
```

위 결과는 복원된 결과를 보여준다. 물론 정수 인코딩을 수행하기 전 어느정도 전처리가 된 상태라서 제대로 된 문장이 나오지는 않는다. 로이터 뉴스 데이터가 어떤 구성을 갖고있는지에 대해서 알아보았다. 텍스트 분류를 수행해보자.

## 2. LSTM으로 로이터 뉴스 분류하기

학습에서는 등장 빈도 순위 상위 1,000개의 단어들만 사용하겠다. 훈련용 뉴스 기사 데이터과 테스트용 뉴스 기사 데이터에 있는 각각의 뉴스의 길이는 서로 다르므로 모든 뉴스 기사의 길이를 100으로 패딩해준다. 이후 훈련용, 테스트용 뉴스 기사 데이터의 레이블에 원-핫 인코딩을 한다.

```javascript
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Embedding
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.models import load_model

vocab_size = 1000
max_len = 100

(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=vocab_size, test_split=0.2)

X_train = pad_sequences(X_train, maxlen=max_len)
X_test = pad_sequences(X_test, maxlen=max_len)

y_train = to_categorical(y_train)
y_test = to_categorical(y_test)
```

하이퍼파라미터인 임베딩 벡터의 차원은 128, 은닉 상태의 크기는 128이다. 단어 집합의 크기는 앞서 1,000으로 정했다. 모델은 다 대 일 구조의 LSTM을 사용한다. 해당 모델은 마지막 시점에서 46개의 선택지 중 하나의 선택지를 예측하는 다중 클래스 분류 문제를 수행하는 모델이다. 다중 클래스 분류 문제의 경우, 출력층에 소프트맥스 회귀를 사용해야 하므로 활성화 함수로는 소프트맥스 함수를 사용하고, 손실 함수로 크로스 엔트로피 함수를 사용한다. 하이퍼파라미터인 배치 크기는 128이며, 30 에포크를 수행한다.

EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)는 검증 데이터 손실(val_loss)이 증가하면, 과적합 징후므로 검증 데이터 손실이 4회 증가하면 정해진 에포크에 도달하지 못하여도 학습을 조기 종료(Early Stopping)한다. ModelCheckpoint를 사용하여 검증 데이터의 정확도(val_acc)가 이전보다 좋아질 경우에만 모델을 저장한다. validation_data로는 X_test와 y_test를 사용한다. val_loss가 줄어들다가 증가하는 상황이 오면 과적합으로 판단하기 위함이다.

```csharp
embedding_dim = 128
hidden_units = 128
num_classes = 46

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim))
model.add(LSTM(hidden_units))
model.add(Dense(num_classes, activation='softmax'))

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)
mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])
history = model.fit(X_train, y_train, batch_size=128, epochs=30, callbacks=[es, mc], validation_data=(X_test, y_test))
```

저자의 경우 21 에포크에서 훈련이 조기 종료되었다. 훈련이 다 되었다면 테스트 데이터에 대해서 정확도를 측정할 차례이다. 훈련 과정에서 검증 데이터가 가장 높았을 때 저장된 모델인 'best_model.h5'를 로드하고, 성능을 평가한다.

```bash
loaded_model = load_model('best_model.h5')
print("\n 테스트 정확도: %.4f" % (loaded_model.evaluate(X_test, y_test)[1]))
```

실행 결과:

```python
2246/2246 [==============================] - 1s 656us/sample - loss: 1.2355 - acc: 0.7124
테스트 정확도: 0.7124
```

테스트 데이터에 대한 정확도는 71.24%이다. 케라스의 model.fit()에서 validation_data는 실제 기계가 데이터를 훈련하지는 않고 에포크마다 정확도와 loss를 출력하여 과적합을 판단하기 위한 용도로만 사용된다. 그래서 validation_data에서 이미 X_test, y_test를 사용했지만 기계는 이 데이터로 학습한 적이 없다. 모델이 학습하지 않은 데이터인 X_test, y_test를 테스트 데이터로서 성능 평가 용도로 model.evaluate()에서도 사용했다. 이번 모델은 검증 데이터와 테스트 데이터가 동일한 셈이다. 사실 데이터가 충분하다면, 검증 데이터와 테스트 데이터는 다르게 사용하는 것이 바람직하다. 앞서 스팸 메일 분류하기 실습에서는 validation_data 대신 validation_split=0.2를 사용하여 검증 데이터와 테스트 데이터를 다른 데이터를 사용하였음을 상기하자. 에포크마다 변화하는 훈련 데이터와 검증 데이터(테스트 데이터)의 손실을 시각화해보자.

```go
epochs = range(1, len(history.history['acc']) + 1)
plt.plot(epochs, history.history['loss'])
plt.plot(epochs, history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
```

![img](https://wikidocs.net/images/page/22933/reuter_news_model_loss.PNG)

전체적으로는 검증 데이터의 손실이 줄어드는 경향이 있지만 뒤로 갈수록 점차 검증 데이터의 손실이 증가하려고 하는 경향이 보인다. 이는 과적합의 신호일 수 있다.