# 텍스트 전처리

## 말뭉치(코퍼스)

- 말뭉치 또는 **코퍼스** (corpus, 복수형 : corpora)
- 자연언어 연구를 위해 특정한 목적을 가지고 언어의 표본을 추출한 집합
- 자연어 처리 관련 애플리케이션은 방대한 양의 데이터
- 코퍼스 분석 뿐만 아니라 언어 분석에 사용되는 실제 언어의 체계적 디지털 모음
- 둘 이상의 코퍼스가 있으면 **코포라**라고 부름
- 코퍼스를 **데이터세트**라고도 함



- 말뭉치(코퍼스)

1. 단일 언어 코퍼스 : 하나의 언어로 이루어짐
2. 이중 언어 코퍼스 : 2개의 언어로 이루어짐
3. 다국어 코퍼스 : 3개 이상의 언어로 이루어짐



텍스트 전처리

- 자연어 처리에서 크롤링 등으로 얻어낸 코퍼스 데이터가 필요에 맞게 전처리되지 않은 상태시 해당 데이터를 용도에 맞게 **토큰화(tokenization) & 정제(cleaning) & 정규화(normalization)**를 진행 해야 함



## 데이터 분석

### 데이터 수집

### 데이터 전처리

- 토큰화
- 정제
- 정규화



- **토큰화**
  - 단어 토큰화(Word Tokenization)
  - 문장 토큰화(Sentence Tokenization)



- 단어 토큰화 (word tokenization)
  - 토큰의 기준을 단어(word)로 하여 토큰화 하는것
  - 단어(word)는 단어 단위 외에도 단어구, 의미를 갖는 문자열로도 간주됨
  - 보통 토큰화 작업은 단순히 구두점이나 특수문자를 전부 제거하는 정제(cleaning) 작업을 수행하는 것만으로 해결되지 않음
  - 구두점이나 특수문자를 전부 제거하면 토큰이 의미를 잃어버리는 경우가 발생함
  - 띄어쓰기 단위로 자를시 단어 토큰 구분이 망가지는 언어도 존재
    - Ex)한국어



입력 : Time is an illusion. Lunchtime double so!

토큰화->

출력 : "Time",  "is", "an", "illusion", "Lunchtime", "double", "so"



- 토큰화 진행 시, 예상하지 못한 경우가 있어서 토큰화의 기준을 생각해봐야 하는 경우가 발생함 Ex) 영어 아포스트로피(apostrophe) 토큰화 문제



- Don`t be fooled by the dark sounding name, Mr Jone's Orphanage is as cheery as cheery goes for a pastry shop
  - Don't
  - Don t
  - Dont
  - Do n't
  - Jone's
  - Jone s
  - Jone
  - Jones

word_tokenize와 WordPunctTokenizer를 사용해서 아포스트로피를 어떻게 처리하는지 확인.

```python
from nltk.tokenize import word_tokenize
from nltk.tokenize import WordPunctTokenizer
from tensorflow.keras.preprocessing.text import text_to_word_sequence
```





```python
print('단어 토큰화1 :',word_tokenize("Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop."))
```

```python
단어 토큰화1 : ['Do', "n't", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', "'s", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']
```



```python
print('단어 토큰화2 :',WordPunctTokenizer().tokenize("Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop."))
```

```python
['Don', "'", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', "'", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']  
```



WordPunctTokenizer는 구두점을 별도로 분류하는 특징을 갖고 있기때문에, 앞서 확인했던 word_tokenize와는 달리 Don't를 Don과 '와 t로 분리하였으며, 이와 마찬가지로 Jone's를 Jone과 '와 s로 분리한 것을 확인할 수 있다. 



케라스 또한 토큰화 도구로서 text_to_word_sequence를 지원한다.

```python
print('단어 토큰화3 :',text_to_word_sequence("Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop."))
```

```python
단어 토큰화3 : ["don't", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', "jone's", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']
```

케라스의 text_to_word_sequence는 기본적으로 모든 알파벳을 소문자로 바꾸면서 마침표나 컴마, 느낌표 등의 구두점을 제거한다. 하지만 don't나 jone's와 같은 경우 아포스트로피는 보존하는 것을 볼 수 있다.



- 토큰화 고려사항
  1. 구두점이나 특수 문자를 단순 제외해서는 안된다.
  2. 줄임말과 단어 내에 띄어쓰기가 있는 경우.



1) **구두점이나 특수 문자를 단순 제외해서는 안된다.**

- 단어들을 걸러낼 때 구두점이나 특수 문자를 단순히 제외하는 것은 옳지 않음
- 구두점조차도 하나의 토큰으로 분류하기도 함

- Ex) 
  - 마침표(.)
    - 문장의 경계를 통한 단어를 추출용 기준이용
  - 단어 자체에 구두점
    - m.p.h나 Ph.D나 AT&T 특수 문자의 달러($)나 슬래시(/)
    - $45.55와 같은 가격을 의미하기도 하고, 01/02/06은 날짜를 의미
  - 숫자 사이에 컴마(,)
    - 123,456,789

2. **줄임말과 단어 내에 띄어쓰기가 있는 경우.**

- 토큰화 작업에서 종종 영어권 언어의 아포스트로피(')는 압축된 단어를 다시 펼치는 역할을 하기도 함
- Ex)
  - what're는 what are의 줄임말
  - we're는 we are의 줄임말
  - New York이라는 단어나 rock 'n' roll
    - 하나의 단어지만 중간에 띄어쓰기가 존재
- **Penn Treebank Tokenization**의 규칙
  - 영어 토큰화의 표준으로 사용되는 규칙

규칙1. 하이픈으로 구성된 단어는 하나로 유지

규칙2. doesn't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리



표준 토큰화 예제

이해를 돕기 위해 표준으로 쓰이고 있는 토큰화 방법 중 하나인 Penn Treebank Tokenization의 규칙에 대해서 소개, 토큰화의 결과를 확인.

규칙 1. 하이푼으로 구성된 단어는 하나로 유지한다.
규칙 2. doesn't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리해준다.

해당 표준에 아래의 문장을 입력으로 넣어본다.
"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own."

```python
from nltk.tokenize import TreebankWordTokenizer

tokenizer = TreebankWordTokenizer()

text = "Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own."
print('트리뱅크 워드토크나이저 :',tokenizer.tokenize(text))
```

```python
트리뱅크 워드토크나이저 : ['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', "n't", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']
```

결과를 보면, 각각 규칙1과 규칙2에 따라서 home-based는 하나의 토큰으로 취급, dosen't의 경우 does와 n't는 분리되었음.



- 문장 토큰화(Sentence Tokenization)

  - 문장 단위로 구분하는 작업으로 때로는 문장 분류(sentence segmentation)

  - 정제되지 않은 상태라면, 코퍼스는 문장 단위로 구분되어 있지 않아서 이를 사용하고자 하는 용도에 맞게 문장 토큰화가 필요

  - !나 ?는 문장의 구분을 위한 꽤 명확한 구분자(boundary) 역할

  - 마침표 . 는 문장의 끝이 아니더라도 등장할 수 있음

    - Ex1)

    IP 192.168.56.31 서버에 들어가서 로그 파일 저장해서 aaa@gmail.com로 결과 좀 보내줘. 그 후 점심 먹으러 가자.

    - Ex2)

    Since I'm activity looking for Ph.D.students, I get the same question a dozen times every year.



NLTK에서는 영어 문장의 토큰화를 수행하는 sent_tokenize를 지원. NLTK를 통해 문장 토큰화를 실습.

```python
from nltk.tokenize import sent_tokenize

text = "His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near."
print('문장 토큰화1 :',sent_tokenize(text))
```

```python
문장 토큰화1 : ['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']
```

위 코드는 text에 저장된 여러 개의 문장들로부터 문장을 구분하는 코드.

 출력 결과를 보면 성공적으로 모든 문장을 구분해내었음을 볼 수 있다. 



문장 중간에 마침표가 다수 등장하는 경우에 대해서도 실습.

```python
text = "I am actively looking for Ph.D. students. and you are a Ph.D student."
print('문장 토큰화2 :',sent_tokenize(text))
```

```python
문장 토큰화2 : ['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']
```

NLTK는 단순히 마침표를 구분자로 하여 문장을 구분하지 않았기 때문에, Ph.D.를 문장 내의 단어로 인식하여 성공적으로 인식하는 것을 볼 수 있다. 



한국어에 대한 문장 토큰화 도구 또한 존재한다. 

```python
import kss

text = '딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?'
print('한국어 문장 토큰화 :',kss.split_sentences(text))
```

```python
한국어 문장 토큰화 : ['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다.', '이제 해보면 알걸요?']
```



- 한국어 토큰화

  - 영어는 New York과 같은 합성어나 he's와 같이 줄임말에 대한 예외처리만 한다면, 띄어쓰기(whitespace)를 기준으로 하는 띄어쓰기 토큰화를 수행해도 단어 토큰화가 잘 작동
  - 거의 대부분의 경우에서 단어 단위로 띄어쓰기가 이루어지기 때문에 띄어쓰기 토큰화와 단어 토큰화가 거의 같기 때문
  - 한국어는 영어와는 달리 띄어쓰기만으로는 토큰화를 하기에 부족
  - 한국어의 경우에는 띄어쓰기 단위가 되는 단위를 '어절'이라고 하는데 어절 토큰화는 한국어 NLP에서 지양되고 있음
  - 어절 토큰화와 단어 토큰화는 다름
  - 한국어가 영어와는 다른 형태를 가지는 언어인 **교착어**라는 점에서 기인
  - **교착어란 조사, 어미 등을 붙여서 말을 만드는 언어**를 말함
  - 영어와는 달리 한국어에는 조사라는 것이 존재
    - Ex) 그라는 단어 하나에도 '그가', '그에게', '그를', '그와', '그는'과 같이 다양한 조사가 '그'라는 글자 뒤에 띄어쓰기 없이 바로 붙게 됨
  - 대부분의 한국어 NLP에서 조사는 분리해줄 필요가 있음
  - 띄어쓰기 단위가 영어처럼 독립적인 단어가 아님
  - 한국어는 어절이 독립적인 단어로 구성되는 것이 아니라 조사 등의 무언가가 붙어있는 경우가 많아서 이를 전부 분리해줘야 함

- 한국어 토큰화에서는 형태소(morpheme)란 개념을 반드시 이해해야 함

- 형태소(morpheme)

  - 뜻을 가진 가장 작은 말의 단위
  - 이 형태소에는 두 가지 형태소가 있는데 자립 형태소와 의존 형태소

- 자립 형태소

  - 접사,  어미, 조사와 상관없이 자립하여 사용할 수 있는 형태소
  - 그 자체로 단어가 됨
  - 체언(명사, 대명사, 수사), 수식언(관형사, 부사), 감탄사 등이 있음

- 의존 형태소

  - 다른 형태소와 결합하여 사용되는 형태소.
  - 접사, 어미, 조사, 어간

- 문장 : 에디가 책을 읽었다

  띄어쓰기 : '에디가', '책을', '읽었다'

  형태소 : 자립형태소 : 에디, 책 		의존 형태소 : -가, -을, 읽-, -었-, -다

- '에디'라는 사람 이름과 '책'이라는 명사를 얻어낼 수 있다.

- 이를 통해 유추할 수 있는 것은 한국어에서의 영어에서의 단어 토큰화와 유사한 형태를 얻으려면 어절 토큰화가 아니라 **형태소 토큰화**를 수행해야 한다.

- 한국어는 띄어쓰기가 영어보다 잘 지켜지지 않는다.

  - 뉴스 기사와 같이  띄어쓰기를 철저하게 지키려고 노력하는 글이라면 좋겠지만, 많은 경우 띄어쓰기가 틀렸거나 지켜지지 않는 코퍼스가 많다
  - 한국어는 영어권 언어와 비교하여 띄어쓰기가 어렵고 잘 지켜지지 않는 경향이 있다
  - 가장 기본적인 견해는 한국어의 경우 띄어쓰기가 지켜지지 않아도 글을 쉽게 이해할 수 있는 언어
  - 띄어쓰기가 없던 한국어에 띄어쓰기가 보편화된 것도 근대(1933년, 한글맞춤법통일안)의 일

- 단어는 표기는 같지만 품사에 따라서 단어의 의미가 달라지기도 함

- 단어의 의미를 제대로 파악하기 위해서는 해당 단어가 어떤 품사로 쓰였는지 보는 것이 주요 지표가 됨

- 품사 태깅(part-of-speech tagging)

  - 단어 토큰화 과정에서 각 단어가 어떤 품사로 쓰였는지를 구분해놓는 작업

NLTK에서는 Penn Treebank POS Tags라는 기준을 사용하여 품사를 태깅.

```python
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag

text = "I am actively looking for Ph.D. students. and you are a Ph.D. student."
tokenized_sentence = word_tokenize(text)

print('단어 토큰화 :',tokenized_sentence)
print('품사 태깅 :',pos_tag(tokenized_sentence)
```

```python
단어 토큰화 : ['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']
품사 태깅 : [('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), ('.', '.'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')]
```



한국어 자연어 처리를 위해서는 KoNLPy(코엔엘파이)라는 파이썬 패키지를 사용. 코엔엘파이를 통해서 사용할 수 있는 형태소 분석기로 Okt(Open Korea Text), 메캅(Mecab), 코모란(Komoran), 한나눔(Hannanum), 꼬꼬마(Kkma)가 있다.

한국어 NLP에서 형태소 분석기를 사용하여 단어 토큰화. 더 정확히는 형태소 토큰화(morpheme tokenization)를 수행.

 여기서는 Okt와 꼬꼬마 두 개의 형태소 분석기를 사용하여 토큰화를 수행.

```python
from konlpy.tag import Okt
from konlpy.tag import Kkma

okt = Okt()
kkma = Kkma()

print('OKT 형태소 분석 :',okt.morphs("열심히 코딩한 당신, 연휴에는 여행을 가봐요"))
print('OKT 품사 태깅 :',okt.pos("열심히 코딩한 당신, 연휴에는 여행을 가봐요"))
print('OKT 명사 추출 :',okt.nouns("열심히 코딩한 당신, 연휴에는 여행을 가봐요")) 
```

```python
OKT 형태소 분석 : ['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']
OKT 품사 태깅 : [('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]
OKT 명사 추출 : ['코딩', '당신', '연휴', '여행']
```

\1) morphs : 형태소 추출
\2) pos : 품사 태깅(Part-of-speech tagging)
\3) nouns : 명사 추출



꼬꼬마 형태소 분석기를 사용하여 같은 문장에 대해서 토큰화를 진행

```python
print('꼬꼬마 형태소 분석 :',kkma.morphs("열심히 코딩한 당신, 연휴에는 여행을 가봐요"))
print('꼬꼬마 품사 태깅 :',kkma.pos("열심히 코딩한 당신, 연휴에는 여행을 가봐요"))
print('꼬꼬마 명사 추출 :',kkma.nouns("열심히 코딩한 당신, 연휴에는 여행을 가봐요")) 
```

```python
꼬꼬마 형태소 분석 : ['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']
꼬꼬마 품사 태깅 : [('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]
꼬꼬마 명사 추출 : ['코딩', '당신', '연휴', '여행']
```



- **정제(cleaning)**

  - 갖고 있는 코퍼스로부터 노이즈 데이터를 제거
  - 완벽한 정제 작업은 어려운 편
  - 대부분의 경우 이 정도면 됐다.라는 일종의 합의점을 찾음

  

- **정규화(normalization)**

  - 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들기

- 규칙에 기반한 통합

  - 정규화 규칙의 예로서 같은 의미를 갖고있음에도, 표기가 다른 단어들을 하나의 단어로 정규화하는 방법을 사용할 수 있음

- 대, 소문자 통합

  - 대부분의 글은 소문자로 작성되기 때문에 대, 소문자 통합 작업은 대부분 대문자를 소문자로 변환하는 소문자 변환작업

- 불필요한 단어의 제거

  - 등장 빈도가 적은 단어
  - 길이가 짧은 단어

- 정규화 기법 중 단어의 개수를 줄일 수 있는 기법

  - **표제어 추출(lemmatization)**
  - **어간 추출(stemming)**

- 하나의 단어로 일반화시킬 수 있다면 하나의 단어로 일반화시켜서 문서 내의 단어 수를 줄이겠다는 것

- 단어의 빈도수를 기반으로 문제를 풀고자 하는 자연어 처리 문제에서 주로 사용

- 자연어 처리에서 전처리 정규화의 지향점은 언제나 갖고 있는 복잡성을 줄이는 일



#### 표제어 추출(Lemmatization)

- '표제어' 또는 '기본 사전형 단어' 정도의 의미
- 표제어 추출은 단어들이 다른 형태를 가지더라도, 그 뿌리 단어를 찾아가서 단어의 개수를 줄일 수 있는지 판단
- 표제어 추출을 하는 가장 섬세한 방법은 단어의 형태학적 파싱을 먼저 진행
- **어간(stem)**
  - 단어의 의미를 담고 있는 단어의 핵심 부분
- **접사(affix)**
  - 단어에 추가적인 의미를 주는 부분
- 형태학적 파싱은 이 두 가지 구성 요소를 분리하는 작업을 말함



표제어 추출 실습

```python
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']

print('표제어 추출 전 :',words)
print('표제어 추출 후 :',[lemmatizer.lemmatize(word) for word in words])
```

```python
표제어 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']
표제어 추출 후 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']
```



#### 어간 추출(Stemming)

- 어간(stem)을 추출하는 작업
- 형태학적 분석을 단순화한 버전
- 정해진 규칙만 보고 단어의 어미를 자르는 어림짐작의 작업
- 섬세한 작업이 아니기 때문에 어간 추출 후에 나오는 결과 단어는 사전에 존재하지 않는 단어일 수도 있음

```python
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

stemmer = PorterStemmer()

sentence = "This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes."
tokenized_sentence = word_tokenize(sentence)

print('어간 추출 전 :', tokenized_sentence)
print('어간 추출 후 :',[stemmer.stem(word) for word in tokenized_sentence])
```

```python
어간 추출 전 : ['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', "'s", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']
어간 추출 후 : ['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', "'s", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']
```



```python
words = ['formalize', 'allowance', 'electricical']

print('어간 추출 전 :',words)
print('어간 추출 후 :',[stemmer.stem(word) for word in words])
```

```python
어간 추출 전 : ['formalize', 'allowance', 'electricical']
어간 추출 후 : ['formal', 'allow', 'electric']
```



포터 알고리즘과 랭커스터 스태머 알고리즘으로 각각 어간 추출을 진행했을 때, 이 둘의 결과를 비교

```python
from nltk.stem import PorterStemmer
from nltk.stem import LancasterStemmer

porter_stemmer = PorterStemmer()
lancaster_stemmer = LancasterStemmer()

words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']
print('어간 추출 전 :', words)
print('포터 스테머의 어간 추출 후:',[porter_stemmer.stem(w) for w in words])
print('랭커스터 스테머의 어간 추출 후:',[lancaster_stemmer.stem(w) for w in words])
```

```python
어간 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']
포터 스테머의 어간 추출 후: ['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']
랭커스터 스테머의 어간 추출 후: ['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']
```



한국어는 5언 9품사의 구조를 가지고 있다.

1. 체언 : 명사, 대명사, 수사
2. 수식언 : 관형사, 부사
3. 관계언 : 조사
4. 독립언 : 감탄사
5. 용언 : 동사, 형용사

- 이 중 용언에 해당되는 '동사'와 '형용사'는 어간(stem)과 어미(ending)의 결합으로 구성



- 동사변화

  - 용언의 어간(stem)이 어미(ending)를 가지는 일
  - 규칙 불규칙 형이 있음

- 어간(stem)

  - 용언(동사, 형용사)을 활용할 때, 원칙적으로 모양이 변하지 않는 부분. 활용에서 어미에 선행하는 부분. 때론 어간의 모양도 바뀔 수 있음(예: 긋다, 긋고, 그어서, 그어라)

- 어미(ending)

  - 용언의 어간 뒤에 붙어서 활용하면서 변하는 부분이며, 여러 문법적 기능을 수행

- 규칙 동사변화

  - 어간이 어미를 취할 때, 어간의 모습이 일정
    - 잡(어간) + 다(어미)
  - 어간이 어미가 붙기 전의 모습과 어미가 붙은 후의 모습이 같으므로, 규칙 기반으로 어미를 단순히 분리해주면 어간 추출이 됨

- 불규칙 동사변화

  - 어간이 어미를 취할 때 어간의 모습이 바뀌거나 취하는 어미가 특수한 어미일 경우

    듣-, 돕-, 곱-, 잇-, 오르-, 노랗- 등이 듣/들-, 돕-도우-, 곱-고우-, 잇/이-, 올/올-, 노랗/노라-

- 어간이 어미가 붙는 과정에서 어간의 모습이 바뀌었으므로 단순한 분리만으로 어간 추출이 되지 않고 좀 더 복잡한 규칙을 필요



- 불용어(stopword)

  - 실제 의미 분석을 하는 데 거의 기여하는 바가 없는 단어들
  - 한국어에서 불용어를 제거하는 방법으로 간단하게는 토큰화 후에 조사, 접속사 등을 제거
  - 사용자가 직접 불용어 사전을 만들게 되는 경우가 많다
  - 불용어가 많은 경우에는 코드 내에서 직접 정의하지 않고 txt파일이나 csv파일로 정리해놓고 이를 불러와서 사용하기도 함



NLTK에서 불용어 확인하기

```python
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize 
from konlpy.tag import Okt
```

```python
stop_words_list = stopwords.words('english')
print('불용어 개수 :', len(stop_words_list))
print('불용어 10개 출력 :',stop_words_list[:10])
```

```python
불용어 개수 : 179
불용어 10개 출력 : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're"]
```



NLTK를 통해서 불용어 제거하기

```python
example = "Family is not an important thing. It's everything."
stop_words = set(stopwords.words('english')) 

word_tokens = word_tokenize(example)

result = []
for word in word_tokens: 
    if word not in stop_words: 
        result.append(word) 

print('불용어 제거 전 :',word_tokens) 
print('불용어 제거 후 :',result
```

```python
불용어 제거 전 : ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', "'s", 'everything', '.']
불용어 제거 후 : ['Family', 'important', 'thing', '.', 'It', "'s", 'everything', '.']
```



한국어에서 불용어 제거하기

직접 불용어를 정의해보고, 주어진 문장으로부터 사용자가 정의한 불용어 사전으로부터 불용어를 제거

```python
okt = Okt()

example = "고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지."
stop_words = "를 아무렇게나 구 우려 고 안 돼 같은 게 구울 때 는"

stop_words = set(stop_words.split(' '))
word_tokens = okt.morphs(example)

result = [word for word in word_tokens if not word in stop_words]

print('불용어 제거 전 :',word_tokens) 
print('불용어 제거 후 :',result)

```

```python
불용어 제거 전 : ['고기', '를', '아무렇게나', '구', '우려', '고', '하면', '안', '돼', '.', '고기', '라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살', '을', '구울', '때', '는', '중요한', '게', '있지', '.']
불용어 제거 후 : ['고기', '하면', '.', '고기', '라고', '다', '아니거든', '.', '예컨대', '삼겹살', '을', '중요한', '있지', '.']
```



- 정규표현식

  - '.' : 한 개의 임의의 문자를 나타낸다(줄바꿈 문자인 \n은 제외)
  - '?' : 앞의 문자가 존재할 수도 있고, 존재하지 않을 수도 있다. (문자가 0개 또는 1개)
  - '*' : 앞의 문자가 무한개로 존재할 수도 있고, 존재하지 않을 수도 있다. (문자가 0개 이상)
  - '+' : 앞의 문자가 최소 한 개 이상 존재한다. (문자가 1개 이상)
  - '^' : 뒤의 문자열로 문자열이 시작된다.
  - '&' : 앞의 문자열로 문자열이 끝난다.
  - '{숫자}' : 숫자만큼 반복한다.
  - '{숫자1, 숫자2}' : 숫자1 이상 숫자2 이하만큼 반복한다. ?, *, +를 이것으로 대체할 수 있다.
  - '{숫자, }' : 숫자 이상만큼 반복한다
  - '[ ]' : 대괄호 안의 문자들 중 한 개의 문자와 매치한다. [amk]라고 한다면 a또는 m또는 k 중 하나라도 존재하면 매치를 의미한다. [a-z]와 같이 범위를 지정할 수도 있다. [a-zA-z]는 알파벳 전체를 의미하는 범위이며, 문자열에 알파벳이 존재하면 매치를 의미한다.
  - [^문자] : 해당 문자를 제외한 문자를 매치한다.
  - | : A|B와 같이 쓰이며 A 또는 B의 의미를 가진다

  ```python
  import re
  text = "I was wondering if anyone out there could enlighten me on this car."
  
  # 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제
  shortword = re.compile(r'\W*\b\w{1,2}\b')
  print(shortword.sub('', text))
  ```

  ```python
  was wondering anyone out there could enlighten this car.
  ```

  

- 정규 표현식 문법에는 역슬래시(\\) 를 이용하여 자주 쓰이는 문자 규칙이 있다.

  - \\\ : 역 슬래시 문자 자체를 의미
  - \d : 모든 숫자를 의미. [0-9]와 동일한 의미 
  - \D : 숫자를 제외한 모든 문자를 의미. [ ^0-9 ] 와 의미가 동일함
  - \s : 공백을 의미. [\t\n\r\f\v]와 동일한 의미
  - \w : 문자 또는 숫자를 의미. [a-zA-Z0-9]와 의미가 동일
  - \W : 문자 또는 숫자가 아닌 문자를 의미 [ ^a-zA-Z0-9 ]와 의미가 동일

  

- 정규표현식 모듈 함수

  - re.compile() : 정규 표현식을 컴파일하는 함수. 다시 말해 파이썬에게 전해주는 역할. 찾고자 하는 패턴이 빈번한 경우에는 미리 컴파일해놓고 사용하면 속도와 편의성면에서 유리함
  - re.search() : 문자열 전체에 대해서 정규표현식과 매치되는지를 검색
  - re.match() : 문자열의 처음이 정규표현식과 매치되는지를 검색
  - re.split() : 정규 표현식을 기준으로 문자열을 분리하여 리스트로 리턴한다.
  - re.findall() : 문자열에서 정규 표현식과 매치되는 모든 경우의 문자열을 찾아서 리스트로 리턴한다. 만약 매치되는 문자열이 없다면 빈 리스트가 리턴된다.
  - re.finditer() : 문자열에서 정규 표현식과 매치되는 모든 경우의 문자열에 대한 이터레이터 객체를 리턴한다.
  - re.sub() : 문자열에서 정규 표현식과 일치하는 부분에 대해서 다른 문자열로 대체한다.
  
  

- 정수 인코딩(Integer Encoding)
  - 각 단어를 고유한 정수에 맵핑(mapping)시키는 전처리 작업
  - 단어를 빈도수 순으로 정렬한 단어 집합(vocabulary)을 만들고, 빈도수가 높은 순서대로 차례로 낮은 숫자부터 정수를 부여하는 방법
  - dictionary
  - Counter



- 패딩(Padding)
  - 병렬 연산을 위해서 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업



- 원-핫 인코딩(One-Hot Encoding)
  - 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식



- 단어 집합 (vocabulary)
  - 기본적으로 book과 books와 같이 단어의 변형 형태도 다른 언어로 간주



- 원-핫 인코딩의 한계
  - 벡터를 저장하기 위해 필요한 공간이 계속 늘어난다는 단점
  - 다른 표현으로는 벡터의 차원이 늘어난다고 표현