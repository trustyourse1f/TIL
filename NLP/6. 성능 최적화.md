# 성능 최적화

## 데이터를 사용한 성능 최적화

- 데이터를 사용한 성능 최적화 방법은 많은 데이터를 수집하는 것
- 데이터 수집이 여의치 않은 상황에서는 임의로 데이터를 생성하는 방법도 고려해 볼 수 있음

## 최대한 많은 데이터 수집하기

- 일반적으로 딥러닝이나 머신 러닝 알고리즘은 데이터 양이 많을수록 성능이 좋음
- 가능한 많은 데이터(빅데이터)를 수집해야 함

![image-20220520103009699](https://user-images.githubusercontent.com/102509786/169435899-71343cbc-6df7-40a0-b156-0e8f11f56a89.png)

## 데이터 생성하기

- 많은 데이터를 수집할 수 없다면 데이터를 만들어 사용할 수 있음

## 데이터 범위 (scale) 조정하기

- 활성화 함수로 시그모이드를 사용한다면 데이터셋 범위를 0~1의 값을 갖도록 하고 하이퍼블릭 탄젠트를 사용한다면 데이터셋 범위를 -1 ~ 1의 값을 갖도록 조정할 수 있음



정규화, 규제화, 표준화도 성능 향상에 도움이 됨



## 알고리즘을 이용한 성능 최적화

- 머신러닝과 딥러닝을 위한 알고리즘은 상당히 많음
- 수많은 알고리즘 중 우리가 선택한 알고리즘이 최적의 알고리즘이 아닐 수도 있음
- 유사한 용도의 알고리즘을 선택하여 모델을 훈련시켜 보고 최적의 성능을 보이는 알고리즘을 선택해야 함
- 머신 러닝에서는 데이터 분류를 위해 SVM, K-최근접 이웃 알고리즘을 선택하여 훈련시켜 보거나 시계열 데이터의 경우 RNN, LSTM, GRU 등의 알고리즘을 훈련시켜 성능이 가장 좋은 모델을 선택하여 사용

### 알고리즘 튜닝을 위한 성능 최적화

- 성능 최적화를 하는 데 가장 많은 시간이 소요되는 부분
- 모델을 하나 선택하여 훈련시키면 다양한 하이퍼파라미터를 변경하면서 훈련시키고 최적의 성능을 도출해야 함

### 진단

- 성능 향상이 어느 순간 멈추었다면 원인을 분석할 필요가 있음
- 문제를 진단하는데 사용할 수 있는 것이 모델에 대한 평가
- 훈련(train) 성능이 검증(test) 성능보다 눈에 띄게 좋다면 과적합을  의심
- 훈련과 검증 결과가 모두 성능이 좋지 않다면 과소적합을 의심
  - 과소적합 상황에서는 네트워크 구조를 변경하거나 훈련을 늘리기 위해 에포크 수를 조정
- 훈련 성능이 검증을 넘어서는 변곡점이 있다면 조기 종료를 고려

### 가중치

- 가중치에 대한 초깃값은 작은 난수를 사용
- 작은 난수라는 숫자가 애매하다면 오토인코더 같은 비지도 학습을 이용하여 사전 훈련(가중치 정보를 얻기 위한 사전 훈련)을 진행한 후 지도 학습을 진행하는 것도 방법

### 학습률

- 학습률은 모델의 네트워크 구성에 따라 다르기 때문에 초기에 매우 크거나 작은 임의의 난수를 선택하여 학습 결과를 보고 조금씩 변경해야 함
  - 네트워크의 계층이 많다면 학습률은 높여야 하며, 네트워크의 계층이 몇 개 되지 않는다면 학습률은 작게 설정해야 함

### 활성화 함수

- 활성화 함수의 변경은 신중해야 함
- 활성화 함수를 변경할 때 손실 함수도 함께 변경해야 하는 경우가 많기 때문
- 일반적으로는 활성화 함수로 시그모이드나 하이퍼블릭 탄젠트를 사용했다면 출력층에서는 소프트맥스나 시그모이드 함수를 많이 선택

### 배치와 에포크

- 일반적으로 큰 에포크와 작은 배치를 사용하는 것이 최근 딥러닝의 트렌드이기는 하지만, 적절한 배치 크기를 위해 훈련 데이터셋의 크기와 동일하게 하거나 하나의 배치로 훈련을 시켜보는 등 다양한 테스트를 진행하는 것이 좋음

### 옵티마이저 및 손실 함수

- 일반적으로 옵티마이저는 확률적 경사 하강법을 많이 사용
- 네트워크 구성에 따라 차이는 있지만 아담(Adam)이나 알엠에스프롭(RMSProp) 등도 좋은 성능을 보이고 있음
- 다양한 옵티마이저와 손실 함수를 적용해 보고 성능이 최고인 것을 선택

### 네트워크 구성

- 네트워크 구성은 네트워크 토폴로지(topology) 라고도 함
- 최적의 네트워크를 구성하는 것 역시 쉽게 알 수 있는 부분이 아니기 때문에 네트워크 구성을 변경해 가면서 성능을 테스트 해야 함
- 하나의 은닉층에 뉴런을 여러 개 포함시키거나 (네트워크가 넓다고 표현) 네트워크 계층을 늘리되 뉴런 개수는 줄여 봄(네트워크가 깊다고 표현)
- 혹은 두 가지를 결합하는 방법으로 최적의 네트뤄크가 무엇인지 확인한 후 사용할 네트워크를 결정해야 함

### 앙상블을 이용한 성능 최적화

- 앙상블은 간단히 모델을 두 개 이상 섞어서 사용하는 것
- 앙상블을 이용하는 것도 성능 향상에 도움이 됨
- 알고리즘 튜닝을 성능 최적화 방법은 하이퍼파라미터에 대한 경우의 수를 모두 고려해야 하기 때문에 모델 훈련이 수십 번에서 수백 번 필요할 수 있음
- 성능 향상은 단시간에 해결되는 것이 아니고 수많은 시행착오를 겪어야 함

### 하이퍼 파라미터를 이용한 성능 최적화

- 배치 정규화, 드롭아웃, 조기종료가 있음

### 배치 정규화를 이용한 성능 최적화

#### 정규화(normalization)

- 데이터 범위를 사용자가 원하는 범위로 제한하는 것을 의미
- 각 특성 범위(스케일)를 조정한다는 의미로 특성 스케일링 이라고도 함

![image-20220520104623519](https://user-images.githubusercontent.com/102509786/169435901-09929050-298a-4779-9e2f-902f1a88a08c.png)

#### 규제화(regularization)

- 모델 복잡도를 줄이기 위해 제약을 두는 방법
- 제약은 데이터가 네트워크에 들어가기 전에 필터를 적용한 것
- 규제를 이용하여 모델 복잡도를 줄이는 방법
- 드롭아웃, 조기 종료

![image-20220520104733867](https://user-images.githubusercontent.com/102509786/169435902-e3cd0626-db44-4c02-8484-4124a052ac2a.png)

#### 표준화(standardization)

- 기존 데이터를 평균은 0, 표준편차는 1인 형태의 데이터로 만드는 방법
- 표준화 스케일링(standard scaling) 혹은 z-스코어 정규화(z-score normalization)

![image-20220520104939755](https://user-images.githubusercontent.com/102509786/169435903-2696b260-7d6f-4eaf-9255-74a827e8146d.png)

#### 배치 정규화 (batch normalization)

- 데이터 분포가 안정되어 학습 속도를 높일 수 있음
- 배치 정규화는 기울기 소멸(gradient vanishing)이나 기울기 폭발(gradient exploding) 같은 문제를 해결하기 위한 방법
- 일반적으로 기울기 소멸이나 폭발 문제를 해결하기 위해 손실함수로 렐루(ReLU)를 사용하거나 초깃값, 튜닝, 학습률 등을 조정
- 단계마다 활성화 함수를 거치면서 데이터셋 분포가 일정해지기 때문에 속도를 향상

##### 기울기 소멸

- 오차 정보를 역전파 시키는 과정에서 기울기가 급격히 0에 가까워져 학습이 되지 않는 현상

##### 기울기 폭발

- 학습 과정에서 기울기가 급격히 커지는 현상

##### 기울기 소멸과 폭발 원인

- 내부 공변량 변화 (internal covariance shift)
- 네트워크의 각 층마다 활성화 함수가 적용되면서 입력 값들의 분포가 계속 바뀌는 현상

![image-20220520105311127](https://user-images.githubusercontent.com/102509786/169435906-019acd70-4698-4753-8924-b528199ab590.png)

### 배치 정규화

- 분산된 분포를 정규 분포로 만들기 위해 표준화와 유사한 방식을 미니 배치에 적용하여 평군은 0으로 표준편차는 1로 유지하도록 함
  1. 미니 배치 평균을 구함
  2. 미니 배치의 분산과 표준편차를 구함
  3. 정규화를 수행
  4. 스케일(scale)을 조정(데이터 분포 조정)함

![image-20220520111321702](https://user-images.githubusercontent.com/102509786/169435908-9e8b6f59-c7e9-4672-8c05-da34893058c9.png)

#### 배치 정규화 단점

- 배치 크기가 작을 때는 정규화 값이 기존 값과 다른 방향으로 훈련될 수 있음
- RNN은 네트워크 계층별로 미니 정규화를 적용해야 하기 떄문에 모델이 더 복잡해지면서 비효율적
- 문제들을 해결하기 위한 가중치 수정, 네트워크 구성 변경 등을 수행
- 배치 정규화를 적용하면 적용하지 않았을 때 보다 성능이 좋아지기 때문에 많이 사용
- 신경망의 층이 깊어질수록 학습할 때 가정했던 입력분포가 변화하여 엉뚱한 학습이 진행될 수 있음
- 배치 정규화를 적용해서 입력 분포를 고르게 맞추어 주면서 과적합을 해소

![image-20220520111612163](https://user-images.githubusercontent.com/102509786/169435910-41faa9c5-ab51-41b3-b733-47fcce50f32f.png)

### 드롭아웃을 이용한 성능 최적화

- 훈련 데이터셋에 대해 훈련을 계속 한다면 오류는 줄어들지만 검증 데이터셋에 대한 오류는 어느 순간부터 증가 -> 과적합 

![image-20220520111739392](https://user-images.githubusercontent.com/102509786/169435914-c43b557e-2fdf-420a-a9f8-22b0252a196e.png)

#### 드롭아웃 (dropout)

- 훈련할 때 일정 비율의 뉴런만 사용하고 나머지 뉴런에 해당하는 가중치는 업데이트 하지 않는 방법
- 매 단계마다 사용하지 않는 뉴런을 바꾸어 가며 훈련시킴
- 노드를 임의로 끄면서 학습하는 방법으로, 은닉층에 배치된 노드 중 일부를 임의로 끄면서 학습
- 꺼진 노드는 신호를 전달하지 않으므로 지나친 학습을 방지
- 적절히 사용되는 드롭아웃은 성능을 향상시키는 데 도움이 되므로 모델 최적화에 활용하기 좋음

![image-20220520111930347](https://user-images.githubusercontent.com/102509786/169435916-a27416dc-f1d2-427e-a7ce-64a30b09805d.png)

### 조기 종료(early stopping)

- 뉴럴 네트워크가 과적합을 회피하는 규제 기법
- 훈련 데이터와 별도로 검증 데이터를 준비하고 매 에포크마다 검증 데이터에 대한 손실(validation loss)을 측정하여 모델의 종료 시점을 제어함
- 조기 종료는 검증에 대한 손실이 증가하는 시점에서 훈련을 멈추도록 조정함

![image-20220520112059012](https://user-images.githubusercontent.com/102509786/169435918-e1897555-1ba8-4635-bba2-0de0a6ca044e.png)