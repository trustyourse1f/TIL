# 6.2 스팸 메일 분류하기(Spam Detection)

캐글에서 제공하는 스팸 메일 데이터를 학습시켜 스팸 메일 분류기를 구현해보자.

## 1. 스팸 메일 데이터에 대한 이해

다운로드 링크 : https://www.kaggle.com/uciml/sms-spam-collection-dataset

```javascript
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import urllib.request
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
```

다운로드 받은 spam.csv 파일을 데이터프레임으로 로드하고 총 샘플의 수를 확인해보자.

```perl
urllib.request.urlretrieve("https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/10.%20RNN%20Text%20Classification/dataset/spam.csv", filename="spam.csv")
data = pd.read_csv('spam.csv', encoding='latin1')
print('총 샘플의 수 :',len(data))
```

실행 결과:

```python
총 샘플의 수 : 5572
```

총 5,572개의 샘플이 존재한다. 상위 5개의 샘플만 출력해보자.

```css
data[:5]
```

![img](https://wikidocs.net/images/page/22894/%ED%9B%88%EB%A0%A8%EB%8D%B0%EC%9D%B4%ED%84%B0.PNG)

스팸 메일 데이터 중에서 5개의 행만 출력해보았다. 이 데이터에는 총 5개의 열이 있는데, 여기서 Unnamed라는 이름의 3개의 열은 텍스트 분류를 할 때 불필요한 열이다. v1열은 해당 메일이 스팸인지 아닌지를 나타내는 레이블에 해당되는 열이다. ham은 정상 메일을 의미하고, spam은 스팸 메일을 의미한다. v2열은 메일의 본문을 담고있다.

레이블과 메일 내용이 담긴 v1열과 v2열만 필요하므로, Unnamed: 2, Unnamed: 3, Unnamed: 4 열은 삭제한다. 또한, v1열에 있는 ham과 spam 레이블을 각각 숫자 0과 1로 바꾸겠다. 다시 data에서 5개의 행만 출력해보자.

```kotlin
del data['Unnamed: 2']
del data['Unnamed: 3']
del data['Unnamed: 4']
data['v1'] = data['v1'].replace(['ham','spam'],[0,1])
data[:5]
```

![img](https://wikidocs.net/images/page/22894/%ED%8E%B8%EC%A7%91%EB%90%A82.PNG)

불필요한 열이 제거되고 v1열의 값이 숫자로 변환된 것을 확인할 수 있다. 해당 데이터프레임의 정보를 확인해보자.

```kotlin
data.info()
```

실행 결과:

```python
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 5572 entries, 0 to 5571
Data columns (total 2 columns):
v1    5572 non-null int64
v2    5572 non-null object
dtypes: int64(1), object(1)
memory usage: 87.1+ KB
```

v1열은 정수형, v2열은 문자열 데이터를 갖고있다. Null 값을 가진 샘플이 있는지 isnull().values.any()로 확인한다.

```python
print('결측값 여부 :',data.isnull().values.any())
```

실행 결과:

```python
결측값 여부 : False
```

False는 별도의 Null 값은 없음을 의미한다. 초기 데이터에 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'열에는 NaN이 있었는데 해당 상태에서 수행하는 isnull().values.any()는 True를 리턴한다. Null 값이 없다면 데이터에 중복이 있는지 확인해보자.

```bash
print('v2열의 유니크한 값 :',data['v2'].nunique())
```

실행 결과:

```python
v2열의 유니크한 값 : 5169
```

총 5,572개의 샘플이 존재하는데 v2열에서 중복을 제거한 샘플의 개수가 5,169개라는 것은 403개의 중복 샘플이 존재한다는 의미이다. 중복 샘플을 제거하고 전체 샘플 수를 확인한다.

```python
# v2 열에서 중복인 내용이 있다면 중복 제거
data.drop_duplicates(subset=['v2'], inplace=True)
print('총 샘플의 수 :',len(data))
```

실행 결과:

```python
총 샘플의 수 : 5169
```

총 샘플의 수가 5,572개에서 5,169개로 줄었다. 레이블 값의 분포를 시각화한다.

```scss
data['v1'].value_counts().plot(kind='bar')
```

![img](https://wikidocs.net/images/page/22894/label_distribution.PNG)

레이블이 대부분 0에 편중되어있는데, 이는 스팸 메일 데이터의 대부분의 메일이 정상 메일임을 의미한다. 정확한 수치를 파악해보자.

```scss
print('정상 메일과 스팸 메일의 개수')
print(data.groupby('v1').size().reset_index(name='count'))
```

실행 결과:

```python
정상 메일과 스팸 메일의 개수
   v1  count
0   0   4516
1   1    653
```

레이블 0은 총 4,516개가 존재하고 1은 653개가 존재한다. 이를 %로 환산한다.

```python
print(f'정상 메일의 비율 = {round(data["v1"].value_counts()[0]/len(data) * 100,3)}%')
print(f'스팸 메일의 비율 = {round(data["v1"].value_counts()[1]/len(data) * 100,3)}%')
```

실행 결과:

```python
정상 메일의 비율 = 87.367%
스팸 메일의 비율 = 12.633%
```

v2열과 v1열을 X데이터와 y데이터라는 X_data, y_data로 저장한다.

```lua
X_data = data['v2']
y_data = data['v1']
print('메일 본문의 개수: {}'.format(len(X_data)))
print('레이블의 개수: {}'.format(len(y_data)))
```

실행 결과:

```python
메일 본문의 개수: 5169
레이블의 개수: 5169
```

훈련 데이터와 테스트 데이터를 분리한다. 주의할 점은 현재 레이블이 굉장히 불균형하다. 다시 말해 정상 메일 샘플(87%, 4516개)에 비해서 스팸 메일 샘플(12%, 653개)이 굉장히 적다. 만약, 훈련 데이터와 테스트 데이터를 분리하는 과정에서 우연히 대부분의 스팸 메일 샘플이 테스트 데이터에 포함되고 훈련 데이터에는 대부분 정상 메일 샘플만 포함되게 된다면 어떻게 될까? 학습 과정에서 모델은 스팸 메일 샘플을 거의 관측하지 못해서 모델의 성능이 저하될 것이다.

이렇게 레이블이 불균형한 경우에는 데이터를 나눌 때에도 훈련 데이터와 테스트 데이터에 각 레이블의 분포가 고르게 분포되도록 하는 것이 중요할 수 있다. 사이킷 런의 train_test_split에 stratify의 인자로서 레이블 데이터를 기재하면 훈련 데이터와 테스트 데이터를 분리할 때 레이블의 분포가 고르게 분포하도록 한다. test_size에 0.2를 기재하여 훈련 데이터와 테스트 데이터를 8:2 비율로 분리한다.

```undefined
X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=0, stratify=y_data)
```

훈련 데이터와 테스트 데이터가 분리되었다. 레이블이 고르게 분포되었는지 확인하자.

```python
print('--------훈련 데이터의 비율-----------')
print(f'정상 메일 = {round(y_train.value_counts()[0]/len(y_train) * 100,3)}%')
print(f'스팸 메일 = {round(y_train.value_counts()[1]/len(y_train) * 100,3)}%')
```

실행 결과:

```python
--------훈련 데이터의 비율-----------
정상 메일 = 87.376%
스팸 메일 = 12.624%
```



```python
print('--------테스트 데이터의 비율-----------')
print(f'정상 메일 = {round(y_test.value_counts()[0]/len(y_test) * 100,3)}%')
print(f'스팸 메일 = {round(y_test.value_counts()[1]/len(y_test) * 100,3)}%')
```

실행 결과:

```python
--------테스트 데이터의 비율-----------
정상 메일 = 87.331%
스팸 메일 = 12.669%
```

훈련 데이터와 테스트 데이터 모두 정상 메일은 87%, 스팸 메일은 12%가 존재한다. 케라스 토크나이저를 통해 훈련 데이터에 대해서 토큰화와 정수 인코딩 과정을 수행한다. X_train_encoded에는 X_train의 각 단어들이 맵핑되는 정수로 인코딩되어 저장된다. 5개의 메일만 출력해서 확인해보자.

```makefile
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)
X_train_encoded = tokenizer.texts_to_sequences(X_train)
print(X_train_encoded[:5])
```

실행 결과:

```python
[[102, 1, 210, 230, 3, 17, 39],
[1, 59, 8, 427, 17, 5, 137, 2, 2326],
[157, 180, 12, 13, 98, 93, 47, 9, 40, 3485, 247, 8, 7, 87, 6, 80, 1312, 5, 3486, 7, 2327, 11, 660, 306, 20, 25, 467, 708, 1028, 203, 129, 193, 800, 2328, 23, 1, 144, 71, 2, 111, 78, 43, 2, 130, 11, 800, 186, 122, 1512],
[1, 1154, 13, 104, 292],
[222, 622, 857, 540, 623, 22, 23, 83, 10, 47, 6, 257, 32, 6, 26, 64, 936, 407]]
```

각 메일이 정수 인코딩이 되었다. 각 정수가 어떤 단어에 부여되었는지 확인해보자.

```bash
word_to_index = tokenizer.word_index
print(word_to_index)
```

실행 결과:

```python
{'i': 1, 'to': 2, 'you': 3, 'a': 4, 'the': 5, 'u': 6, 'and': 7, 'in': 8, 'is': 9, 'me': 10, 'my': 11, 'for': 12, 'your': 13, 'it': 14, 'of': 15, 'have': 16, 'on': 17, 'call': 18, 'that': 19, 'are': 20, '2': 21, 'now': 22, 'so': 23, 'but': 24, 'not': 25, 'can': 26, 'or': 27, "i'm": 28, 'get': 29, 'at': 30, 'do': 31, 'if': 32, 'be': 33, 'will': 34, 'just': 35, 'with': 36, 'we': 37, 'no': 38, 'this': 39, 'ur': 40, 'up': 41, '4': 42, 'how': 43, 'gt': 44, 'lt': 45, 'go': 46, 'when': 47, 'from': 48, 'what': 49, 'ok': 50, 'out': 51, 'know': 52, - 이하 생략}
```

무수히 많은 단어가 출력되므로 출력 결과는 중간에 생략했다. 위에서 부여된 각 정수는 각 단어의 빈도수가 높을 수록 낮은 정수가 부여된다. 다시 말해, 단어 i는 현재 전체 훈련 데이터에서 빈도수가 가장 높은 단어이다.

각 단어에 대한 등장 빈도수는 tokenizer.word_counts.items()를 출력해서 확인할 수 있다. 이를 응용하여 빈도수가 낮은 단어들이 훈련 데이터에서 얼만큼의 비중을 차지하는지 확인해볼 수 있다. 등장 빈도수가 1회 밖에 되지 않는 단어들이 전체 단어 집합에서 얼만큼의 비율을 차지하며, 전체 훈련 데이터에서 등장 빈도로 얼만큼의 비율을 차지하는지 확인해보자.

```makefile
threshold = 2
total_cnt = len(word_to_index) # 단어의 수
rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트
total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합
rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합

# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.
for key, value in tokenizer.word_counts.items():
    total_freq = total_freq + value

    # 단어의 등장 빈도수가 threshold보다 작으면
    if(value < threshold):
        rare_cnt = rare_cnt + 1
        rare_freq = rare_freq + value

print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))
print("단어 집합(vocabulary)에서 희귀 단어의 비율:", (rare_cnt / total_cnt)*100)
print("전체 등장 빈도에서 희귀 단어 등장 빈도 비율:", (rare_freq / total_freq)*100)
```

실행 결과:

```python
등장 빈도가 1번 이하인 희귀 단어의 수: 4337
단어 집합(vocabulary)에서 희귀 단어의 비율: 55.45326684567191
전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 6.65745644331875
```

등장 빈도가 threshold 값인 2회 미만. 즉, 1회 밖에 되지 않는 단어들은 단어 집합에서 무려 절반 이상을 차지한다. 하지만, 실제로 훈련 데이터에서 등장 빈도로 차지하는 비중은 상대적으로 적은 수치인 6%밖에 되지 않는다. 만약, 이러한 분석을 통해 등장 빈도가 지나치게 낮은 단어들은 자연어 처리에서 제외하고 싶다면 케라스 토크나이저 선언 시에 단어 집합의 크기를 제한할 수 있다. 가령, 아래의 코드로 등장 빈도가 1회인 단어들을 제외할 수 있을 것이다.

- tokenizer = Tokenizer(num_words = total_cnt - rare_cnt + 1)

하지만 이번 실습에서는 별도로 단어 집합의 크기를 제한하진 않겠다. 단어 집합의 크기를 vocab_size에 저장하겠다. 패딩을 위한 토큰인 0번 단어를 고려하며 +1을 해서 저장한다.

```lua
vocab_size = len(word_to_index) + 1
print('단어 집합의 크기: {}'.format((vocab_size)))
```

실행 결과:

```python
단어 집합의 크기: 7822
```

단어 집합의 크기는 7,822이다. 전체 데이터에서 가장 길이가 긴 메일과 전체 메일 데이터의 길이 분포를 확인한다.

```python
print('메일의 최대 길이 : %d' % max(len(sample) for sample in X_train_encoded))
print('메일의 평균 길이 : %f' % (sum(map(len, X_train_encoded))/len(X_train_encoded)))
plt.hist([len(sample) for sample in X_data], bins=50)
plt.xlabel('length of samples')
plt.ylabel('number of samples')
plt.show()
```

실행 결과:

```python
메일의 최대 길이 : 189
메일의 평균 길이 : 15.754534
```



![img](https://wikidocs.net/images/page/22894/sample_distribution.PNG)

가장 긴 메일의 길이는 189이며, 전체 데이터의 길이 분포는 대체적으로 약 50이하의 길이를 가진다.

```makefile
max_len = 189
X_train_padded = pad_sequences(X_train_encoded, maxlen = max_len)
print("훈련 데이터의 크기(shape):", X_train_padded.shape)
```

maxlen에는 가장 긴 메일의 길이였던 189이라는 숫자를 넣었다. 이는 4,135개의 X_train_encoded의 길이를 전부 189로 바꾼다. 189보다 길이가 짧은 메일 샘플은 전부 숫자 0이 패딩되어 189의 길이를 가진다.

```scss
훈련 데이터의 크기(shape): (4135, 189)
```

X_train_encoded 데이터는 4,135 × 189의 크기를 갖게된다. 모델을 설계해보자.

## 2. RNN으로 스팸 메일 분류하기

하이퍼파라미터인 임베딩 벡터의 차원은 32, 은닉 상태의 크기는 32이다. 모델은 다 대 일 구조의 RNN을 사용한다. 해당 모델은 마지막 시점에서 두 개의 선택지 중 하나를 예측하는 이진 분류 문제를 수행하는 모델이다. 이진 분류 문제의 경우, 출력층에 로지스틱 회귀를 사용해야 하므로 활성화 함수로는 시그모이드 함수를 사용하고, 손실 함수로 크로스 엔트로피 함수를 사용하여 4번의 에포크를 수행한다.

하이퍼파라미터인 배치 크기는 64이며, validation_split=0.2을 사용하여 훈련 데이터의 20%를 검증 데이터로 분리해서 사용하고, 검증 데이터를 통해서 훈련이 적절히 되고 있는지 확인한다. 검증 데이터는 기계가 훈련 데이터에 과적합되고 있지는 않은지 확인하기 위한 용도로 사용된다.

```csharp
from tensorflow.keras.layers import SimpleRNN, Embedding, Dense
from tensorflow.keras.models import Sequential

embedding_dim = 32
hidden_units = 32

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim))
model.add(SimpleRNN(hidden_units))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
history = model.fit(X_train_padded, y_train, epochs=4, batch_size=64, validation_split=0.2)
```

테스트 데이터에 대해서 정확도를 확인해보자.

```makefile
X_test_encoded = tokenizer.texts_to_sequences(X_test)
X_test_padded = pad_sequences(X_test_encoded, maxlen = max_len)
print("\n 테스트 정확도: %.4f" % (model.evaluate(X_test_padded, y_test)[1]))
```

실행 결과:

```python
1034/1034 [==============================] - 0s 166us/step
테스트 정확도: 0.9821
```

정확도가 98%가 나왔다. 이번 실습에서는 훈련 데이터와 검증 데이터에 대해서 같이 정확도를 확인하면서 훈련하였으므로, 이를 비교하여 그래프로 시각화해보자.

```go
epochs = range(1, len(history.history['acc']) + 1)
plt.plot(epochs, history.history['loss'])
plt.plot(epochs, history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()
```

![img](https://wikidocs.net/images/page/22894/%EC%8A%A4%ED%8C%B8%EB%A9%94%EC%9D%BC%EC%98%A4%EC%B0%A8.png)

이번 실습 데이터는 데이터의 양이 적어 과적합이 빠르게 시작되므로, 검증 데이터에 대한 오차가 증가하기 시작하는 시점의 바로 직전인 에포크 3~4 정도가 적당하다. 이 데이터는 에포크 5를 넘어가기 시작하면 검증 데이터의 오차가 증가하는 경향이 있다.