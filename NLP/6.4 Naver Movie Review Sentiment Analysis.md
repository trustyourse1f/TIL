# 6.4 네이버 영화 리뷰 감성 분류하기(Naver Movie Review Sentiment Analysis)

이번에 사용할 데이터는 네이버 영화 리뷰 데이터이다. 총 200,000개 리뷰로 구성된 데이터로 영화 리뷰에 대한 텍스트와 해당 리뷰가 긍정인 경우 1, 부정인 경우 0을 표시한 레이블로 구성되어져 있다. 해당 데이터를 다운로드 받아 감성 분류를 수행하는 모델을 만들어보겠다.

## **1. 네이버 영화 리뷰 데이터에 대한 이해와 전처리**

데이터 다운로드 링크 : https://github.com/e9t/nsmc/

```javascript
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
import urllib.request
from konlpy.tag import Okt
from tqdm import tqdm
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
```

### **1) 데이터 로드하기**

위 링크로부터 훈련 데이터에 해당하는 ratings_train.txt와 테스트 데이터에 해당하는 ratings_test.txt를 다운로드한다.

```bash
urllib.request.urlretrieve("https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt", filename="ratings_train.txt")
urllib.request.urlretrieve("https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt", filename="ratings_test.txt")
```

pandas를 이용하여 훈련 데이터는 train_data에 테스트 데이터는 test_data에 저장한다.

```ini
train_data = pd.read_table('ratings_train.txt')
test_data = pd.read_table('ratings_test.txt')
```

train_data에 존재하는 영화 리뷰의 개수를 확인해보자.

```python
print('훈련용 리뷰 개수 :',len(train_data)) # 훈련용 리뷰 개수 출력
```

실행 결과:

```python
훈련용 리뷰 개수 : 150000
```

train_data는 총 150,000개의 리뷰가 존재한다. 상위 5개의 샘플을 출력해보자.

```bash
train_data[:5] # 상위 5개 출력
```

![img](https://wikidocs.net/images/page/44249/navermovie1.PNG)

해당 데이터는 id, document, label 총 3개의 열로 구성되어져 있다. id는 감성 분류를 수행하는데 도움이 되지 않으므로 앞으로 무시한다. 결국 이 모델은 리뷰 내용을 담고있는 document와 해당 리뷰가 긍정(1), 부정(0)인지를 나타내는 label 두 개의 열을 학습하는 모델이 되어야 한다.

또한 단지 상위 5개의 샘플만 출력해보았지만 한국어 데이터와 영어 데이터의 차이를 확인할 수 있다. 예를 들어, 인덱스 2번 샘플은 띄어쓰기를 하지 않아도 글을 쉽게 이해할 수 있는 한국어의 특성으로 인해 띄어쓰기가 되어있지 않다. test_data의 리뷰 개수와 상위 5개의 샘플을 확인해보자.

```python
print('테스트용 리뷰 개수 :',len(test_data)) # 테스트용 리뷰 개수 출력
```

실행 결과:

```python
테스트용 리뷰 개수 : 50000
```

test_data는 총 50,000개의 영화 리뷰가 존재한다. 상위 5개의 샘플을 출력해보자.

```css
test_data[:5]
```

![img](https://wikidocs.net/images/page/44249/navermovie2.PNG)

test_data도 train_data와 동일한 형식으로 id, document, label 3개의 열로 구성되어져 있다.

### **2) 데이터 정제하기**

train_data의 데이터 중복 유무를 확인한다.

```scss
# document 열과 label 열의 중복을 제외한 값의 개수
train_data['document'].nunique(), train_data['label'].nunique()
```

실행 결과:

```python
(146182, 2)
```

총 150,000개의 샘플이 존재하는데 document열에서 중복을 제거한 샘플의 개수가 146,182개라는 것은 약 4,000개의 중복 샘플이 존재한다는 의미이다. label 열은 0 또는 1의 두 가지 값만을 가지므로 2가 출력된다. 중복 샘플을 제거한다.

```python
# document 열의 중복 제거
train_data.drop_duplicates(subset=['document'], inplace=True)
```

중복 샘플을 제거하였다. 중복이 제거되었는지 전체 샘플 수를 확인한다.

```go
print('총 샘플의 수 :',len(train_data))
```

실행 결과:

```python
총 샘플의 수 : 146183
```

중복 샘플이 제거되었다. train_data에서 해당 리뷰의 긍, 부정 유무가 기재되어있는 레이블(label) 값의 분포를 보자.

```scss
train_data['label'].value_counts().plot(kind = 'bar')
```

![img](https://wikidocs.net/images/page/44249/label_distribution.PNG)

앞서 확인하였듯이 약 146,000개의 영화 리뷰 샘플이 존재하는데 그래프 상으로 긍정과 부정 둘 다 약 72,000개의 샘플이 존재하여 레이블의 분포가 균일한 것처럼 보인다. 정확하게 몇 개인지 확인해보자.

```scss
print(train_data.groupby('label').size().reset_index(name = 'count'))
```

실행 결과:

```python
   label  count
0      0  73342
1      1  72841
```

레이블이 0인 리뷰가 근소하게 많다. 리뷰 중에 Null 값을 가진 샘플이 있는지 확인한다.

```scss
print(train_data.isnull().values.any())
```

실행 결과:

```python
True
```

True가 나왔다면 데이터 중에 Null 값을 가진 샘플이 존재한다는 의미이다. 어떤 열에 존재하는지 확인해보자.

```scss
print(train_data.isnull().sum())
```

실행 결과:

```python
id          0
document    1
label       0
dtype: int64
```

리뷰가 적혀있는 document 열에서 Null 값을 가진 샘플이 총 1개가 존재한다고 한다. 그렇다면 document 열에서 Null 값이 존재한다는 것을 조건으로 Null 값을 가진 샘플이 어느 인덱스의 위치에 존재하는지 한 번 출력해보자.

```css
train_data.loc[train_data.document.isnull()]
```

![img](https://wikidocs.net/images/page/44249/navermoive4new.PNG)

출력 결과는 위와 같다. Null 값을 가진 샘플을 제거하겠다.

```python
train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거
print(train_data.isnull().values.any()) # Null 값이 존재하는지 확인
```

실행 결과:

```python
False
```

Null 값을 가진 샘플이 제거되었다. 다시 샘플의 개수를 출력하여 1개의 샘플이 제거되었는지 확인해보자.

```go
print(len(train_data))
```

실행 결과:

```python
146182
```

데이터의 전처리를 수행해보겠다. 위의 train_data와 test_data에서 온점(.)이나 ?와 같은 각종 특수문자가 사용된 것을 확인했다. train_data로부터 한글만 남기고 제거하기 위해서 정규 표현식을 사용해보겠다.

우선 영어를 예시로 정규 표현식을 설명해보겠다. 영어의 알파벳들을 나타내는 정규 표현식은 [a-zA-Z]이다. 이 정규 표현식은 영어의 소문자와 대문자들을 모두 포함하고 있는 정규 표현식으로 이를 응용하면 영어에 속하지 않는 구두점이나 특수문자를 제거할 수 있다. 예를 들어 알파벳과 공백을 제외하고 모두 제거하는 전처리를 수행하는 예제는 다음과 같다.

```python
#알파벳과 공백을 제외하고 모두 제거
eng_text = 'do!!! you expect... people~ to~ read~ the FAQ, etc. and actually accept hard~! atheism?@@'
print(re.sub(r'[^a-zA-Z ]', '', eng_text))
```

실행 결과:

```python
'do you expect people to read the FAQ etc and actually accept hard atheism'
```

위와 같은 원리를 한국어 데이터에 적용하고 싶다면, 우선 한글을 범위 지정할 수 있는 정규 표현식을 찾아내면 되겠다. 우선 자음과 모음에 대한 범위를 지정해보겠다. 일반적으로 자음의 범위는 ㄱ ~ ㅎ, 모음의 범위는 ㅏ ~ ㅣ와 같이 지정할 수 있다. 해당 범위 내에 어떤 자음과 모음이 속하는지 알고 싶다면 아래의 링크를 참고하기 바란다.

링크 : https://www.unicode.org/charts/PDF/U3130.pdf
ㄱ ~ ㅎ: 3131 ~ 314E
ㅏ ~ ㅣ: 314F ~ 3163

완성형 한글의 범위는 가 ~ 힣과 같이 사용한다. 해당 범위 내에 포함된 음절들은 아래의 링크에서 확인할 수 있다.

링크 : https://www.unicode.org/charts/PDF/UAC00.pdf

위 범위 지정을 모두 반영하여 train_data에 한글과 공백을 제외하고 모두 제거하는 정규 표현식을 수행해보자.

```python
# 한글과 공백을 제외하고 모두 제거
train_data['document'] = train_data['document'].str.replace("[^ㄱ-ㅎㅏ-ㅣ가-힣 ]","")
train_data[:5]
```

![img](https://wikidocs.net/images/page/44249/navermovie5.PNG)

상위 5개의 샘플을 다시 출력해보았는데, 정규 표현식을 수행하자 기존의 공백. 즉, 띄어쓰기는 유지되면서 온점과 같은 구두점 등은 제거되었다. 사실 네이버 영화 리뷰는 한글이 아니더라도 영어, 숫자, 특수문자로도 리뷰를 업로드할 수 있다. 다시 말해 기존에 한글이 없는 리뷰였다면 더 이상 아무런 값도 없는 빈(empty) 값이 되었을 것이다. train_data에 공백(whitespace)만 있거나 빈 값을 가진 행이 있다면 Null 값으로 변경하도록 하고, Null 값이 존재하는지 확인해보자.

```python
train_data['document'] = train_data['document'].str.replace('^ +', "") # white space 데이터를 empty value로 변경
train_data['document'].replace('', np.nan, inplace=True)
print(train_data.isnull().sum())
```

실행 결과:

```python
id            0
document    789
label         0
dtype: int64
```

Null 값이 789개나 새로 생겼다. Null 값이 있는 행을 5개만 출력해보자.

```css
train_data.loc[train_data.document.isnull()][:5]
```

![img](https://wikidocs.net/images/page/44249/top_null_data.PNG)

Null 샘플들은 레이블이 긍정일 수도 있고, 부정일 수도 있다. 아무런 의미도 없는 데이터므로 제거해준다.

```go
train_data = train_data.dropna(how = 'any')
print(len(train_data))
```

실행 결과:

```python
145393
```

샘플 개수가 또 다시 줄어서 145,393개가 남았다. 테스트 데이터에 앞서 진행한 전처리 과정을 동일하게 진행한다.

```python
test_data.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거
test_data['document'] = test_data['document'].str.replace("[^ㄱ-ㅎㅏ-ㅣ가-힣 ]","") # 정규 표현식 수행
test_data['document'] = test_data['document'].str.replace('^ +', "") # 공백은 empty 값으로 변경
test_data['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경
test_data = test_data.dropna(how='any') # Null 값 제거
print('전처리 후 테스트용 샘플의 개수 :',len(test_data))
```

실행 결과:

```python
전처리 후 테스트용 샘플의 개수 : 48852
```



### **3) 토큰화**

토큰화를 진행해보자. 토큰화 과정에서 불용어를 제거하겠다. 불용어는 정의하기 나름인데, 한국어의 조사, 접속사 등의 보편적인 불용어를 사용할 수도 있겠지만 결국 풀고자 하는 문제의 데이터를 지속 검토하면서 계속해서 추가하는 경우 또한 많다. 실제 현업인 상황이라면 일반적으로 아래의 불용어보다 더 많은 불용어를 사용할 수 있다.

```ini
stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']
```

여기서는 위 정도로만 불용어를 정의하고, 토큰화를 위한 형태소 분석기는 KoNLPy의 Okt를 사용한다. Okt를 복습해보자.

```python
okt = Okt()
okt.morphs('와 이런 것도 영화라고 차라리 뮤직비디오를 만드는 게 나을 뻔', stem = True)
```

실행 결과:

```python
['오다', '이렇다', '것', '도', '영화', '라고', '차라리', '뮤직비디오', '를', '만들다', '게', '나다', '뻔']
```

Okt는 위와 같이 KoNLPy에서 제공하는 형태소 분석기이다. 한국어을 토큰화할 때는 영어처럼 띄어쓰기 기준으로 토큰화를 하는 것이 아니라, 주로 형태소 분석기를 사용한다고 언급한 바 있다. stem = True를 사용하면 일정 수준의 정규화를 수행해주는데, 예를 들어 위의 예제의 결과를 보면 '이런'이 '이렇다'로 변환되었고 '만드는'이 '만들다'로 변환된 것을 알 수 있다. train_data에 형태소 분석기를 사용하여 토큰화를 하면서 불용어를 제거하여 X_train에 저장한다.

```csharp
X_train = []
for sentence in tqdm(train_data['document']):
    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화
    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거
    X_train.append(stopwords_removed_sentence)
```

상위 3개의 샘플만 출력하여 결과를 확인해보자.

```scss
print(X_train[:3])
```

실행 결과:

```python
[['아', '더빙', '진짜', '짜증나다', '목소리'], ['흠', '포스터', '보고', '초딩', '영화', '줄', '오버', '연기', '조차', '가볍다', '않다'], ['너', '무재', '밓었', '다그', '래서', '보다', '추천', '다']]
```

형태소 토큰화가 진행된 것을 볼 수 있다. 테스트 데이터에 대해서도 동일하게 토큰화를 해준다.

```csharp
X_test = []
for sentence in tqdm(test_data['document']):
    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화
    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거
    X_test.append(stopwords_removed_sentence)
```

지금까지 훈련 데이터와 테스트 데이터에 대해서 텍스트 전처리를 진행해보았다.

### **4) 정수 인코딩**

기계가 텍스트를 숫자로 처리할 수 있도록 훈련 데이터와 테스트 데이터에 정수 인코딩을 수행해야 한다. 우선, 훈련 데이터에 대해서 단어 집합(vocaburary)을 만들어보자.

```makefile
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)
```

단어 집합이 생성되는 동시에 각 단어에 고유한 정수가 부여되었다. 이는 tokenizer.word_index를 출력하여 확인 가능하다.

```scss
print(tokenizer.word_index)
```

실행 결과:

```python
{'영화': 1, '보다': 2, '을': 3, '없다': 4, '이다': 5, '있다': 6, '좋다': 7, ... 중략 ... '디케이드': 43751, '수간': 43752}
```

단어가 43,000개가 넘게 존재한다. 각 정수는 전체 훈련 데이터에서 등장 빈도수가 높은 순서대로 부여되었기 때문에, 높은 정수가 부여된 단어들은 등장 빈도수가 매우 낮다는 것을 의미한다. 여기서는 빈도수가 낮은 단어들은 자연어 처리에서 배제하고자 한다. 등장 빈도수가 3회 미만인 단어들이 이 데이터에서 얼만큼의 비중을 차지하는지 확인해보자.

```python
threshold = 3
total_cnt = len(tokenizer.word_index) # 단어의 수
rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트
total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합
rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합

# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.
for key, value in tokenizer.word_counts.items():
    total_freq = total_freq + value

    # 단어의 등장 빈도수가 threshold보다 작으면
    if(value < threshold):
        rare_cnt = rare_cnt + 1
        rare_freq = rare_freq + value

print('단어 집합(vocabulary)의 크기 :',total_cnt)
print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))
print("단어 집합에서 희귀 단어의 비율:", (rare_cnt / total_cnt)*100)
print("전체 등장 빈도에서 희귀 단어 등장 빈도 비율:", (rare_freq / total_freq)*100)
```

실행 결과:

```python
단어 집합(vocabulary)의 크기 : 43752
등장 빈도가 2번 이하인 희귀 단어의 수: 24337
단어 집합에서 희귀 단어의 비율: 55.62488571950996
전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 1.8715872104872904
```

등장 빈도가 threshold 값인 3회 미만. 즉, 2회 이하인 단어들은 단어 집합에서 무려 절반 이상을 차지한다. 하지만, 실제로 훈련 데이터에서 등장 빈도로 차지하는 비중은 상대적으로 매우 적은 수치인 1.87%밖에 되지 않는다. 아무래도 등장 빈도가 2회 이하인 단어들은 자연어 처리에서 별로 중요하지 않을 듯 하다. 그래서 이 단어들은 정수 인코딩 과정에서 배제시키겠다.

등장 빈도수가 2이하인 단어들의 수를 제외한 단어의 개수를 단어 집합의 최대 크기로 제한하겠다.

```bash
# 전체 단어 개수 중 빈도수 2이하인 단어는 제거.
# 0번 패딩 토큰을 고려하여 + 1
vocab_size = total_cnt - rare_cnt + 1
print('단어 집합의 크기 :',vocab_size)
```

실행 결과:

```python
단어 집합의 크기 : 19416
```

단어 집합의 크기는 19,416개이다. 이를 케라스 토크나이저의 인자로 넘겨주고 텍스트 시퀀스를 정수 시퀀스로 변환한다.

```makefile
tokenizer = Tokenizer(vocab_size) 
tokenizer.fit_on_texts(X_train)
X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)
```

정수 인코딩이 진행되었는지 확인하고자 X_train에 대해서 상위 3개의 샘플만 출력한다.

```scss
print(X_train[:3])
```

실행 결과:

```python
[[50, 454, 16, 260, 659], [933, 457, 41, 602, 1, 214, 1449, 24, 961, 675, 19], [386, 2444, 2315, 5671, 2, 222, 9]]
```

각 샘플 내의 단어들은 각 단어에 대한 정수로 변환된 것을 확인할 수 있다. 단어의 개수는 19,416개로 제한되었으므로 0번 단어 ~ 19,415번 단어까지만 사용 중이다. 0번 단어는 패딩을 위한 토큰임을 상기하자. train_data에서 y_train과 y_test를 별도로 저장해준다.

```ini
y_train = np.array(train_data['label'])
y_test = np.array(test_data['label'])
```

### **5) 빈 샘플(empty samples) 제거**

전체 데이터에서 빈도수가 낮은 단어가 삭제되었다는 것은 빈도수가 낮은 단어만으로 구성되었던 샘플들은 빈(empty) 샘플이 되었다는 것을 의미한다. 빈 샘플들은 어떤 레이블이 붙어있던 의미가 없으므로 빈 샘플들을 제거해주는 작업을 하겠다. 각 샘플들의 길이를 확인해서 길이가 0인 샘플들의 인덱스를 받아오겠다.

```ini
drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]
```

drop_train에는 X_train으로부터 얻은 빈 샘플들의 인덱스가 저장된다. 앞서 훈련 데이터(X_train, y_train)의 샘플 개수는 145,791개임을 확인했었다. 그렇다면 빈 샘플들을 제거한 후의 샘플 개수는 몇 개일까?

```go
# 빈 샘플들을 제거
X_train = np.delete(X_train, drop_train, axis=0)
y_train = np.delete(y_train, drop_train, axis=0)
print(len(X_train))
print(len(y_train))
```

실행 결과:

```python
145162
145162
```

145,162개로 샘플의 수가 줄어든 것을 확인할 수 있다.

### **6) 패딩**

서로 다른 길이의 샘플들의 길이를 동일하게 맞춰주는 패딩 작업을 진행해보겠다. 전체 데이터에서 가장 길이가 긴 리뷰와 전체 데이터의 길이 분포를 알아보겠다.

```python
print('리뷰의 최대 길이 :',max(len(review) for review in X_train))
print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))
plt.hist([len(review) for review in X_train], bins=50)
plt.xlabel('length of samples')
plt.ylabel('number of samples')
plt.show()
```

실행 결과:

```python
리뷰의 최대 길이 : 69
리뷰의 평균 길이 : 10.812485361182679
```



![img](https://wikidocs.net/images/page/44249/new_review_distributions.PNG)

가장 긴 리뷰의 길이는 69이며, 그래프를 봤을 때 전체 데이터의 길이 분포는 대체적으로 약 11내외의 길이를 가지는 것을 볼 수 있다. 모델이 처리할 수 있도록 X_train과 X_test의 모든 샘플의 길이를 특정 길이로 동일하게 맞춰줄 필요가 있다. 특정 길이 변수를 max_len으로 정한다. 대부분의 리뷰가 내용이 잘리지 않도록 할 수 있는 최적의 max_len의 값은 몇일까? 전체 샘플 중 길이가 max_len 이하인 샘플의 비율이 몇 %인지 확인하는 함수를 만든다.

```python
def below_threshold_len(max_len, nested_list):
  count = 0
  for sentence in nested_list:
    if(len(sentence) <= max_len):
        count = count + 1
  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (count / len(nested_list))*100))
```

위의 분포 그래프를 봤을 때, max_len = 30이 적당할 것 같다. 이 값이 얼마나 많은 리뷰 길이를 커버하는지 확인해보자.

```makefile
max_len = 30
below_threshold_len(max_len, X_train)
```

실행 결과:

```python
전체 샘플 중 길이가 30 이하인 샘플의 비율: 94.31944999380003
```

전체 훈련 데이터 중 약 94%의 리뷰가 30이하의 길이를 가지는 것을 확인했다. 모든 샘플의 길이를 30으로 맞추겠다.

```ini
X_train = pad_sequences(X_train, maxlen=max_len)
X_test = pad_sequences(X_test, maxlen=max_len)
```

## **2. LSTM으로 네이버 영화 리뷰 감성 분류하기**

하이퍼파라미터인 임베딩 벡터의 차원은 100, 은닉 상태의 크기는 128이다. 모델은 다 대 일 구조의 LSTM을 사용한다. 해당 모델은 마지막 시점에서 두 개의 선택지 중 하나를 예측하는 이진 분류 문제를 수행하는 모델이다. 이진 분류 문제의 경우, 출력층에 로지스틱 회귀를 사용해야 하므로 활성화 함수로는 시그모이드 함수를 사용하고, 손실 함수로 크로스 엔트로피 함수를 사용한다. 하이퍼파라미터인 배치 크기는 64이며, 15 에포크를 수행한다.

EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)는 검증 데이터 손실(val_loss)이 증가하면, 과적합 징후므로 검증 데이터 손실이 4회 증가하면 정해진 에포크가 도달하지 못하였더라도 학습을 조기 종료(Early Stopping)한다는 의미이다. ModelCheckpoint를 사용하여 검증 데이터의 정확도(val_acc)가 이전보다 좋아질 경우에만 모델을 저장한다. validation_split=0.2을 사용하여 훈련 데이터의 20%를 검증 데이터로 분리해서 사용하고, 검증 데이터를 통해서 훈련이 적절히 되고 있는지 확인한다. 검증 데이터는 기계가 훈련 데이터에 과적합되고 있지는 않은지 확인하기 위한 용도로 사용된다.

```python
from tensorflow.keras.layers import Embedding, Dense, LSTM
from tensorflow.keras.models import Sequential
from tensorflow.keras.models import load_model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

embedding_dim = 100
hidden_units = 128

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim))
model.add(LSTM(hidden_units))
model.add(Dense(1, activation='sigmoid'))

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)
mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)
```

조기 종료 조건에 따라서 9 에포크에서 훈련이 멈췄다. 훈련이 다 되었다면 테스트 데이터에 대해서 정확도를 측정할 차례이다. 훈련 과정에서 검증 데이터의 정확도가 가장 높았을 때 저장된 모델인 'best_model.h5'를 로드한다.

```bash
loaded_model = load_model('best_model.h5')
print("\n 테스트 정확도: %.4f" % (loaded_model.evaluate(X_test, y_test)[1]))
```

실행 결과:

```python
테스트 정확도: 0.8544
```

테스트 데이터에서 85.44%의 정확도를 얻었다.

## **3. 리뷰 예측해보기**

임의의 리뷰에 대해서 예측하는 함수를 만들어보겠다. 기본적으로 현재 학습한 model에 새로운 입력에 대해서 예측값을 얻는 것은 model.predict()를 사용한다. 그리고 model.fit()을 할 때와 마찬가지로 새로운 입력에 대해서도 동일한 전처리를 수행 후에 model.predict()의 입력으로 사용해야 한다.

```python
def sentiment_predict(new_sentence):
  new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)
  new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화
  new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거
  encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩
  pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩
  score = float(loaded_model.predict(pad_new)) # 예측
  if(score > 0.5):
    print("{:.2f}% 확률로 긍정 리뷰입니다.\n".format(score * 100))
  else:
    print("{:.2f}% 확률로 부정 리뷰입니다.\n".format((1 - score) * 100))
```

```python
sentiment_predict('이 영화 개꿀잼 ㅋㅋㅋ')
```

실행 결과:

```python
97.76% 확률로 긍정 리뷰입니다.
```



```python
sentiment_predict('이 영화 핵노잼 ㅠㅠ')
```

실행 결과:

```python
98.55% 확률로 부정 리뷰입니다.
```



```python
sentiment_predict('이딴게 영화냐 ㅉㅉ')
```

실행 결과:

```python
99.91% 확률로 부정 리뷰입니다.
```



```python
sentiment_predict('감독 뭐하는 놈이냐?')
```

실행 결과:

```python
98.21% 확률로 부정 리뷰입니다.
```



```python
sentiment_predict('와 개쩐다 정말 세계관 최강자들의 영화다')
```

실행 결과:

```python
80.77% 확률로 긍정 리뷰입니다.
```

