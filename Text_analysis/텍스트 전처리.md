# 텍스트 전처리

## 말뭉치(코퍼스)

- 말뭉치 또는 **코퍼스** (corpus, 복수형 : corpora)
- 자연언어 연구를 위해 특정한 목적을 가지고 언어의 표본을 추출한 집합
- 자연어 처리 관련 애플리케이션은 방대한 양의 데이터
- 코퍼스 분석 뿐만 아니라 언어 분석에 사용되는 실제 언어의 체계적 디지털 모음
- 둘 이상의 코퍼스가 있으면 **코포라**라고 부름
- 코퍼스를 **데이터세트**라고도 함



- 말뭉치(코퍼스)

1. 단일 언어 코퍼스 : 하나의 언어로 이루어짐
2. 이중 언어 코퍼스 : 2개의 언어로 이루어짐
3. 다국어 코퍼스 : 3개 이상의 언어로 이루어짐



텍스트 전처리

- 자연어 처리에서 크롤링 등으로 얻어낸 코퍼스 데이터가 필요에 맞게 전처리되지 않은 상태시 해당 데이터를 용도에 맞게 **토큰화(tokenization) & 정제(cleaning) & 정규화(normalization)**를 진행 해야 함



## 데이터 분석

### 데이터 수집

### 데이터 전처리

- 토큰화
- 정제
- 정규화



- **토큰화**
  - 단어 토큰화(Word Tokenization)
  - 문장 토큰화(Sentence Tokenization)



- 단어 토큰화 (word tokenization)
  - 토큰의 기준을 단어(word)로 하여 토큰화 하는것
  - 단어(word)는 단어 단위 외에도 단어구, 의미를 갖는 문자열로도 간주됨
  - 보통 토큰화 작업은 단순히 구두점이나 특수문자를 전부 제거하는 정제(cleaning) 작업을 수행하는 것만으로 해결되지 않음
  - 구두점이나 특수문자를 전부 제거하면 토큰이 의미를 잃어버리는 경우가 발생함
  - 띄어쓰기 단위로 자를시 단어 토큰 구분이 망가지는 언어도 존재
    - Ex)한국어



입력 : Time is an illusion. Lunchtime double so!

토큰화->

출력 : "Time",  "is", "an", "illusion", "Lunchtime", "double", "so"



- 토큰화 진행 시, 예상하지 못한 경우가 있어서 토큰화의 기준을 생각해봐야 하는 경우가 발생함 Ex) 영어 아포스트로피(apostrophe) 토큰화 문제



- Don`t be fooled by the dark sounding name, Mr Jone's Orphanage is as cheery as cheery goes for a pastry shop
  - Don't
  - Don t
  - Dont
  - Do n't
  - Jone's
  - Jone s
  - Jone
  - Jones



- 토큰화 고려사항
  1. 구두점이나 특수 문자를 단순 제외해서는 안된다.
  2. 줄임말과 단어 내에 띄어쓰기가 있는 경우.



1) **구두점이나 특수 문자를 단순 제외해서는 안된다.**

- 단어들을 걸러낼 때 구두점이나 특수 문자를 단순히 제외하는 것은 옳지 않음
- 구두점조차도 하나의 토큰으로 분류하기도 함

- Ex) 
  - 마침표(.)
    - 문장의 경계를 통한 단어를 추출용 기준이용
  - 단어 자체에 구두점
    - m.p.h나 Ph.D나 AT&T 특수 문자의 달러($)나 슬래시(/)
    - $45.55와 같은 가격을 의미하기도 하고, 01/02/06은 날짜를 의미
  - 숫자 사이에 컴마(,)
    - 123,456,789

2. **줄임말과 단어 내에 띄어쓰기가 있는 경우.**

- 토큰화 작업에서 종종 영어권 언어의 아포스트로피(')는 압축된 단어를 다시 펼치는 역할을 하기도 함
- Ex)
  - what're는 what are의 줄임말
  - we're는 we are의 줄임말
  - New York이라는 단어나 rock 'n' roll
    - 하나의 단어지만 중간에 띄어쓰기가 존재
- **Penn Treebank Tokenization**의 규칙
  - 영어 토큰화의 표준으로 사용되는 규칙

규칙1. 하이픈으로 구성된 단어는 하나로 유지

규칙2. doesn't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리



- 문장 토큰화(Sentence Tokenization)

  - 문장 단위로 구분하는 작업으로 때로는 문장 분류(sentence segmentation)

  - 정제되지 않은 상태라면, 코퍼스는 문장 단위로 구분되어 있지 않아서 이를 사용하고자 하는 용도에 맞게 문장 토큰화가 필요

  - !나 ?는 문장의 구분을 위한 꽤 명확한 구분자(boundary) 역할

  - 마침표 . 는 문장의 끝이 아니더라도 등장할 수 있음

    - Ex1)

    IP 192.168.56.31 서버에 들어가서 로그 파일 저장해서 aaa@gmail.com로 결과 좀 보내줘. 그 후 점심 먹으러 가자.

    - Ex2)

    Since I'm activity looking for Ph.D.students, I get the same question a dozen times every year.

    

- 한국어 토큰화

  - 영어는 New York과 같은 합성어나 he's와 같이 줄임말에 대한 예외처리만 한다면, 띄어쓰기(whitespace)를 기준으로 하는 띄어쓰기 토큰화를 수행해도 단어 토큰화가 잘 작동
  - 거의 대부분의 경우에서 단어 단위로 띄어쓰기가 이루어지기 때문에 띄어쓰기 토큰화와 단어 토큰화가 거의 같기 때문
  - 한국어는 영어와는 달리 띄어쓰기만으로는 토큰화를 하기에 부족
  - 한국어의 경우에는 띄어쓰기 단위가 되는 단위를 '어절'이라고 하는데 어절 토큰화는 한국어 NLP에서 지양되고 있음
  - 어절 토큰화와 단어 토큰화는 다름
  - 한국어가 영어와는 다른 형태를 가지는 언어인 **교착어**라는 점에서 기인
  - **교착어란 조사, 어미 등을 붙여서 말을 만드는 언어**를 말함
  - 영어와는 달리 한국어에는 조사라는 것이 존재
    - Ex) 그라는 단어 하나에도 '그가', '그에게', '그를', '그와', '그는'과 같이 다양한 조사가 '그'라는 글자 뒤에 띄어쓰기 없이 바로 붙게 됨
  - 대부분의 한국어 NLP에서 조사는 분리해줄 필요가 있음
  - 띄어쓰기 단위가 영어처럼 독립적인 단어가 아님
  - 한국어는 어절이 독립적인 단어로 구성되는 것이 아니라 조사 등의 무언가가 붙어있는 경우가 많아서 이를 전부 분리해줘야 함

- 한국어 토큰화에서는 형태소(morpheme)란 개념을 반드시 이해해야 함

- 형태소(morpheme)

  - 뜻을 가진 가장 작은 말의 단위
  - 이 형태소에는 두 가지 형태소가 있는데 자립 형태소와 의존 형태소

- 자립 형태소

  - 접사,  어미, 조사와 상관없이 자립하여 사용할 수 있는 형태소
  - 그 자체로 단어가 됨
  - 체언(명사, 대명사, 수사), 수식언(관형사, 부사), 감탄사 등이 있음

- 의존 형태소

  - 다른 형태소와 결합하여 사용되는 형태소.
  - 접사, 어미, 조사, 어간

- 문장 : 에디가 책을 읽었다

  띄어쓰기 : '에디가', '책을', '읽었다'

  형태소 : 자립형태소 : 에디, 책 		의존 형태소 : -가, -을, 읽-, -었-, -다

- '에디'라는 사람 이름과 '책'이라는 명사를 얻어낼 수 있다.

- 이를 통해 유추할 수 있는 것은 한국어에서의 영어에서의 단어 토큰화와 유사한 형태를 얻으려면 어절 토큰화가 아니라 **형태소 토큰화**를 수행해야 한다.

- 한국어는 띄어쓰기가 영어보다 잘 지켜지지 않는다.

  - 뉴스 기사와 같이  띄어쓰기를 철저하게 지키려고 노력하는 글이라면 좋겠지만, 많은 경우 띄어쓰기가 틀렸거나 지켜지지 않는 코퍼스가 많다
  - 한국어는 영어권 언어와 비교하여 띄어쓰기가 어렵고 잘 지켜지지 않는 경향이 있다
  - 가장 기본적인 견해는 한국어의 경우 띄어쓰기가 지켜지지 않아도 글을 쉽게 이해할 수 있는 언어
  - 띄어쓰기가 없던 한국어에 띄어쓰기가 보편화된 것도 근대(1933년, 한글맞춤법통일안)의 일

- 단어는 표기는 같지만 품사에 따라서 단어의 의미가 달라지기도 함

- 단어의 의미를 제대로 파악하기 위해서는 해당 단어가 어떤 품사로 쓰였는지 보는 것이 주요 지표가 됨

- 품사 태깅(part-of-speech tagging)

  - 단어 토큰화 과정에서 각 단어가 어떤 품사로 쓰였는지를 구분해놓는 작업



- **정제(cleaning)**

  - 갖고 있는 코퍼스로부터 노이즈 데이터를 제거
  - 완벽한 정제 작업은 어려운 편
  - 대부분의 경우 이 정도면 됐다.라는 일종의 합의점을 찾음

  

- **정규화(normalization)**

  - 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들기

- 규칙에 기반한 통합

  - 정규화 규칙의 예로서 같은 의미를 갖고있음에도, 표기가 다른 단어들을 하나의 단어로 정규화하는 방법을 사용할 수 있음

- 대, 소문자 통합

  - 대부분의 글은 소문자로 작성되기 때문에 대, 소문자 통합 작업은 대부분 대문자를 소문자로 변환하는 소문자 변환작업

- 불필요한 단어의 제거

  - 등장 빈도가 적은 단어
  - 길이가 짧은 단어

- 정규화 기법 중 단어의 개수를 줄일 수 있는 기법

  - **표제어 추출(lemmatization)**
  - **어간 추출(stemming)**

- 하나의 단어로 일반화시킬 수 있다면 하나의 단어로 일반화시켜서 문서 내의 단어 수를 줄이겠다는 것

- 단어의 빈도수를 기반으로 문제를 풀고자 하는 자연어 처리 문제에서 주로 사용

- 자연어 처리에서 전처리 정규화의 지향점은 언제나 갖고 있는 복잡성을 줄이는 일



#### 표제어 추출(Lemmatization)

- '표제어' 또는 '기본 사전형 단어' 정도의 의미
- 표제어 추출은 단어들이 다른 형태를 가지더라도, 그 뿌리 단어를 찾아가서 단어의 개수를 줄일 수 있는지 판단
- 표제어 추출을 하는 가장 섬세한 방법은 단어의 형태학적 파싱을 먼저 진행

- **어간(stem)**
  - 단어의 의미를 담고 있는 단어의 핵심 부분
- **접사(affix)**
  - 단어에 추가적인 의미를 주는 부분
- 형태학적 파싱은 이 두 가지 구성 요소를 분리하는 작업을 말함



#### 어간 추출(Stemming)

- 어간(stem)을 추출하는 작업
- 형태학적 분석을 단순화한 버전
- 정해진 규칙만 보고 단어의 어미를 자르는 어림짐작의 작업
- 섬세한 작업이 아니기 때문에 어간 추출 후에 나오는 결과 단어는 사전에 존재하지 않는 단어일 수도 있음



한국어는 5언 9품사의 구조를 가지고 있다.

1. 체언 : 명사, 대명사, 수사
2. 수식언 : 관형사, 부사
3. 관계언 : 조사
4. 독립언 : 감탄사
5. 용언 : 동사, 형용사

- 이 중 용언에 해당되는 '동사'와 '형용사'는 어간(stem)과 어미(ending)의 결합으로 구성



- 동사변화

  - 용언의 어간(stem)이 어미(ending)를 가지는 일
  - 규칙 불규칙 형이 있음

- 어간(stem)

  - 용언(동사, 형용사)을 활용할 때, 원칙적으로 모양이 변하지 않는 부분. 활용에서 어미에 선행하는 부분. 때론 어간의 모양도 바뀔 수 있음(예: 긋다, 긋고, 그어서, 그어라)

- 어미(ending)

  - 용언의 어간 뒤에 붙어서 활용하면서 변하는 부분이며, 여러 문법적 기능을 수행

- 규칙 동사변화

  - 어간이 어미를 취할 때, 어간의 모습이 일정
    - 잡(어간) + 다(어미)
  - 어간이 어미가 붙기 전의 모습과 어미가 붙은 후의 모습이 같으므로, 규칙 기반으로 어미를 단순히 분리해주면 어간 추출이 됨

- 불규칙 동사변화

  - 어간이 어미를 취할 때 어간의 모습이 바뀌거나 취하는 어미가 특수한 어미일 경우

    듣-, 돕-, 곱-, 잇-, 오르-, 노랗- 등이 듣/들-, 돕-도우-, 곱-고우-, 잇/이-, 올/올-, 노랗/노라-

- 어간이 어미가 붙는 과정에서 어간의 모습이 바뀌었으므로 단순한 분리만으로 어간 추출이 되지 않고 좀 더 복잡한 규칙을 필요



- 불용어(stopword)

  - 실제 의미 분석을 하는 데 거의 기여하는 바가 없는 단어들
  - 한국어에서 불용어를 제거하는 방법으로 간단하게는 토큰화 후에 조사, 접속사 등을 제거
  - 사용자가 직접 불용어 사전을 만들게 되는 경우가 많다
  - 불용어가 많은 경우에는 코드 내에서 직접 정의하지 않고 txt파일이나 csv파일로 정리해놓고 이를 불러와서 사용하기도 함

- 정규표현식

  - '.' : 한 개의 임의의 문자를 나타낸다(줄바꿈 문자인 \n은 제외)
  - '?' : 앞의 문자가 존재할 수도 있고, 존재하지 않을 수도 있다. (문자가 0개 또는 1개)
  - '*' : 앞의 문자가 무한개로 존재할 수도 있고, 존재하지 않을 수도 있다. (문자가 0개 이상)
  - '+' : 앞의 문자가 최소 한 개 이상 존재한다. (문자가 1개 이상)
  - '^' : 뒤의 문자열로 문자열이 시작된다.
  - '&' : 앞의 문자열로 문자열이 끝난다.
  - '{숫자}' : 숫자만큼 반복한다.
  - '{숫자1, 숫자2}' : 숫자1 이상 숫자2 이하만큼 반복한다. ?, *, +를 이것으로 대체할 수 있다.
  - '{숫자, }' : 숫자 이상만큼 반복한다
  - '[ ]' : 대괄호 안의 문자들 중 한 개의 문자와 매치한다. [amk]라고 한다면 a또는 m또는 k 중 하나라도 존재하면 매치를 의미한다. [a-z]와 같이 범위를 지정할 수도 있다. [a-zA-z]는 알파벳 전체를 의미하는 범위이며, 문자열에 알파벳이 존재하면 매치를 의미한다.
  - [^문자] : 해당 문자를 제외한 문자를 매치한다.
  - | : A|B와 같이 쓰이며 A 또는 B의 의미를 가진다

  

- 정규 표현식 문법에는 역슬래시(\\) 를 이용하여 자주 쓰이는 문자 규칙이 있다.

  - \\\ : 역 슬래시 문자 자체를 의미
  - \d : 모든 숫자를 의미. [0-9]와 동일한 의미 
  - \D : 숫자를 제외한 모든 문자를 의미. [ ^0-9 ] 와 의미가 동일함
  - \s : 공백을 의미. [\t\n\r\f\v]와 동일한 의미
  - \w : 문자 또는 숫자를 의미. [a-zA-Z0-9]와 의미가 동일
  - \W : 문자 또는 숫자가 아닌 문자를 의미 [ ^a-zA-Z0-9 ]와 의미가 동일

  

- 정규표현식 모듈 함수

  - re.compile() : 정규 표현식을 컴파일하는 함수. 다시 말해 파이썬에게 전해주는 역할. 찾고자 하는 패턴이 빈번한 경우에는 미리 컴파일해놓고 사용하면 속도와 편의성면에서 유리함
  - re.search() : 문자열 전체에 대해서 정규표현식과 매치되는지를 검색
  - re.match() : 문자열의 처음이 정규표현식과 매치되는지를 검색
  - re.split() : 정규 표현식을 기준으로 문자열을 분리하여 리스트로 리턴한다.
  - re.findall() : 문자열에서 정규 표현식과 매치되는 모든 경우의 문자열을 찾아서 리스트로 리턴한다. 만약 매치되는 문자열이 없다면 빈 리스트가 리턴된다.
  - re.finditer() : 문자열에서 정규 표현식과 매치되는 모든 경우의 문자열에 대한 이터레이터 객체를 리턴한다.
  - re.sub() : 문자열에서 정규 표현식과 일치하는 부분에 대해서 다른 문자열로 대체한다.



- 정수 인코딩(Integer Encoding)
  - 각 단어를 고유한 정수에 맵핑(mapping)시키는 전처리 작업
  - 단어를 빈도수 순으로 정렬한 단어 집합(vocabulary)을 만들고, 빈도수가 높은 순서대로 차례로 낮은 숫자부터 정수를 부여하는 방법
  - dictionary
  - Counter



- 패딩(Padding)
  - 병렬 연산을 위해서 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업



- 원-핫 인코딩(One-Hot Encoding)
  - 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식



- 단어 집합 (vocabulary)
  - 기본적으로 book과 books와 같이 단어의 변형 형태도 다른 언어로 간주



- 원-핫 인코딩의 한계
  - 벡터를 저장하기 위해 필요한 공간이 계속 늘어난다는 단점
  - 다른 표현으로는 벡터의 차원이 늘어난다고 표현