# 딥러닝

## 신경망

- 최근에 많은 인기를 끌고 있는 딥러닝(deep learning)의 시작은 1950년대부터 연구되어 온 인공 신경망(artificial neural network : ANN)
- 인공 신경망은 생물학적인 신경망에서 영감을 받아서 만들어진 컴퓨팅 구조이다.

## 뉴런

- 가지돌기에 들어온 여러 신호는 하나로 통합되어 어떤 임계값을 초과하면 축삭돌기를 통해 이동되고 다음 신경세포에 전달

![image-20220522142327172](https://user-images.githubusercontent.com/102509786/169681060-5877140d-a610-4f89-bf02-f299a5a4f250.png)

- 인공 신경망의 장점

  - 데이터만 주어지면 신경망은 예제로부터 배울 수 있다.
  - 몇 개의 소자가 오작동 하더라도 전체적으로는 큰 문제가 발생하지 않는다.

  ![image-20220522142520251](https://user-images.githubusercontent.com/102509786/169681062-483a140d-f250-48c5-b488-218b9e260185.png)

## 퍼셉트론

- 가장 간단한 인공 신경망 구조 중 하나로 1957년에 프랑크 로젠블라트가 제안
- 다수의 신호를 입력으로 받아 하나의 신호를 출력
- 퍼셉트론 신호도 흐름을 만들고 정보를 앞으로 전달
- 다만 실제 전류와 달리 퍼셉트론 신호는 흐른다/안흐른다(1이나 0)의 두 가지 값을 가진다.
- 1을 신호가 흐른다. 0을 신호가 흐르지 않는다 라는 의미로 쓰인다.

![image-20220522142648122](https://user-images.githubusercontent.com/102509786/169681063-8261a122-c01f-49e4-8216-229ac23ddce3.png)

![image-20220522142652230](https://user-images.githubusercontent.com/102509786/169681064-31252907-2285-47bb-82fe-dbbac5a4a1b2.png)

![image-20220522142702787](https://user-images.githubusercontent.com/102509786/169681065-5d19eaa0-1cfd-4c1a-8502-621214a79881.png)



- 퍼셉트론
  - 다수 입력(input), 하나의 출력(output)
  - 가중치(weights)
    - 입력의 강도(중용도)를 표현
  - 스스로 학습하는 능력
    - 초기 가중치는 임의의 값 지정
    - 뉴런의 결과를 목표값과 비교
    - 그 차이인 오차(error)를 사용
    - 다시 그 다음 단계의 가중치에 반영



임계값 -b

- 가중치와 입력 곱의 합
  - 가중 합계 (weighted sum)
  - w0 + x1w1 + x2w2 + ... + xnwn
- 값이 임계값(threshold) b(=-w0)보다 크면
  - 뉴런은 실행(fire), 결과값이 1
- 그렇지 않으면
  - 0이 결과값



학습이라고 부르려면 신경망이 스스로 가중치를 자동으로 설정해주는 알고리즘이 필요하다. 퍼셉트론에서도 학습 알고리즘이 존재한다.



가중치와 편향을 도입한 AND게이트

```python
import numpy as np
def AND(x1, x2):
    x = np.array([x1, x2])
    w = np.array([0.5, 0.5])
    b = -0.7
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1
```

NAND 게이트와 OR게이트

```python
def NAND(x1, x2):
    x = np.array([x1, x2])
    w = np.array([-0.5, -0.5])
    b = 0.7
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1
    
def OR(x1, x2):
    x = np.array([x1, x2])
    w = np.array([0.5, 0.5])
    b = -0.2
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1
    
```



## 퍼셉트론의 한계

- 퍼셉트론은 직선 하나로 나눈 영역만 표현할 수 있다는 한계가 있다.

- 패턴 인식 측면에서 보면 퍼셉트론은 직선을 이용하여 입력 패턴을 분류하는 선형 분류자 (linear classifier)의 일종이라고 할 수 있다.

  ![image-20220522144601917](https://user-images.githubusercontent.com/102509786/169681067-25a8c5f8-f02b-467f-a13a-47ff0396e10d.png)

![image-20220522144707553](https://user-images.githubusercontent.com/102509786/169681068-cbeb2289-5c09-40ca-a115-724312370fce.png)



- 뉴런은 다른 뉴런들로부터 신호를 받아서 모두 합한 후에 비선형 함수를 적용하여 출력을 계산한다.
- 연결선은 가중치를 가지고 있고 이 가중치에 학습의 결과가 저장된다.
- 퍼셉트론은 하나의 뉴런만을 사용한다. 다수의 입력을 받아서 하나의 신호를 출력하는 장치이다.
- 퍼셉트론은 AND나 OR같은 논리적인 연산을 학습할 수 있었지만 XOR 연산은 학습할 수 없었다. 선형 분리 가능한 문제만을 학습할 수 있었다.



단층 퍼셉트론으로는 표현하지 못한 것을 층을 하나 늘려 구현할 수 있다.

```python
def XOR(x1, x2):
    s1 = NAND(x1, x2)
    s2 = OR(x1, x2)
    y = AND(s1, s2)
    return y
```



![image-20220522145353175](https://user-images.githubusercontent.com/102509786/169681069-97203d99-d147-4a98-a5ac-e97ec406a601.png)

가장 왼쪽 줄을 입력층, 맨 오른쪽 줄을 출력층, 중간 줄을 은닉층이라 한다.

은닉층의 뉴런은 입력층이나 출력층과 달리 사람 눈에는 보이지 않는다.



h(x)라는 함수가 등장했는데 이처럼 입력 신호의 총합을 출력 신호로 변환하는 함수를 일반적으로 activation function이라 한다.

활성화 라는 이름이 말해주듯 활성화 함수는 입력 신호의 총합이 활성화를 일으키는지를 정하는 역할



## 다층 퍼셉트론 (multilayer perceptron:MLP)

: 입력층과 출력층 사이에 은닉층 (hidden layer)을 가지고 있는 신경망

![image-20220522145543915](https://user-images.githubusercontent.com/102509786/169681070-6045decf-b605-46a0-b8d8-0e8ef8f47411.png)



역전파 알고리즘은 입력이 주어지면 순방향으로 계산하여 출력을 계산한 후에 실제 출력과 우리가 원하는 출력 간의 오차를 계산

이 오차를 역방향으로 전파하면서 오차를 줄이는 방향으로 가중치를 변경

![image-20220522145631708](https://user-images.githubusercontent.com/102509786/169681071-4aa01e15-bc46-469b-b1cf-441dccbd3030.png)