# 오차 역전파

- 역전파 알고리즘은 입력이 주어지면 순방향으로 계산하여 출력을 계산한 후에 실제  출력과 우리가 원하는 출력 간의 오차를 계산한다.
- 이 오차를 역방향으로 전파하면서 오차를 줄이는 방향으로 가중치를 변경한다.

![image-20220522152556091](https://user-images.githubusercontent.com/102509786/169683124-b5f4ea25-9824-4254-88c3-86b6ee8135f8.png)

- 손실함수
  - 전체 오차는 목표 출력값에서 실제 출력값을 빼서 제곱한 값을 모든 출력 노드에 대하여 합한 값이다.

![image-20220522152856504](https://user-images.githubusercontent.com/102509786/169683127-c5bc3610-0b38-44a3-be62-6829e7e65ef4.png)

## 경사 하강법

- 현재 위치에서 함수의 그래디언트 값을 계산한 후에 그래디언트의 반대 방향으로 움직이는 방법

![image-20220522152945417](https://user-images.githubusercontent.com/102509786/169683128-dbf693e1-4804-418c-8b31-7bbc2947c4c1.png)

- 손실 함수 : ![image-20220522152958240](https://user-images.githubusercontent.com/102509786/169683130-c87ecb33-9618-487a-930e-f643baf77816.png)

- 가중치의 변경 : ![image-20220522153008655](https://user-images.githubusercontent.com/102509786/169683131-c63fb6e0-2d68-40ea-a37f-4f58f361ae8b.png)

- 우리가 계산해야 할 것 :![image-20220522153022595](https://user-images.githubusercontent.com/102509786/169683133-cc47d613-ca58-4d48-ae07-b468ea6a105d.png)

![image-20220522153201300](https://user-images.githubusercontent.com/102509786/169683135-2beba1a1-19b0-4fb4-af9d-0ccce9e46db6.png)

## 역전파 알고리즘

- 신경망의 가중치를 작은 난수로 초기화한다.
- do 각 훈련 샘플 sample 에 대하여 다음을 반복
  - actual = calculate_network(sample) //순방향 패스
  - target = desired_output(sample)
  - 각 출력 노드에서 오차(target - actual)을 계산
  - 은닉층에서 출력층으로의 가중치 변경값을 계산. //역방행 패스
  - 입력층에서 은닉층으로의 가중치 변경값을 계산. //역방향 패스
  - 전체 가중치를 업데이트
- until 모든 샘플이 올바르게 분류될 때 까지





수식 wx+b 에서 가중치 w를 기울기 a로 생각하면 결국 ax+b가 되어 기울기가 a, y절편이 b인 1차 함수와 같음

- 입력 특성은 x 하나이고 w를 b로 표현
  - 다음과 같은 간단한 수식의 퍼셉트론
- 입력 특성이 하나
  - 가중치는 기울기, 편향은 절편

![image-20220522153625481](https://user-images.githubusercontent.com/102509786/169683137-81a0bc82-bae0-4a4d-95ee-3478489eea59.png)

활성화 함수는 퍼셉트론의 최종 값의 세기를 조절하는 데 사용되며 활성화 함수 출력값은 다음 퍼셉트론의 입력으로 사용

퍼셉트론에서 최종 값을 결정하는 함수

- 퍼셉트론의 입력값과 가중치를 곱한 것에 편향을 모두 합한 값에 대한 함수

## 활성화 함수

- 내부에서 입력받은 데이터를 근거로 다음 계층으로 출력할 값을 결정
- 신경망을 구성할 때 설정하며 각각의 레이어를 정의할 때 세부적인 함수를 선택
- 은닉 계층의 활성화 함수

출력 계층의 활성화 함수 : 목표에 부합하는 함수를 선택

| 해당계층 | 딥러닝목표 | 활성화 함수  |
| -------- | ---------- | ------------ |
| 은닉계층 | -          | relu/sigmoid |
| 출력계층 | 이진분류   | sigmoid      |
|          | 다중분류   | softmax      |

### 단위 계단 함수 (unit step function)

- 로젠블랫의 활성화 함수
- 가중 합계(weighted sum)
  - 0이상이면 결과는 1
  - 0미만이면 0

![image-20220522154206145](https://user-images.githubusercontent.com/102509786/169683139-3d2db54c-c5cc-459f-997d-fe79c0eac884.png)

```python
def step_function(x):
    if x > 0:
        return 1
    else:
        return 0
```

### 항등 함수 (identify function)

가장 단순한 활성화 함수는 입력 그대로 출력하는 f(x) = x

### ReLU 함수

양수 부분은 항등 함수와 동일, 0 이하는 0

```python
def relu(x):
    return np.maximum(0, x)
```

![image-20220522154651677](https://user-images.githubusercontent.com/102509786/169683140-41f6b446-5d94-43b2-bd04-1410a69cb6cb.png)

### 시그모이드(sigmoid) 함수와 하이퍼블릭탄젠트(tanh) 함수

```python
def sigmoid(x):
    return 1/ (1 + np.exp(-x))
```



## 다층 신경망(MLP Multi-Layer Perceptron)

- 인공 신경망을 확장해 가로로 여러 개 연결한 층(layer)을 쌓음
- 은닉층(hidden layer) 또는 중간층
  - 입력층과 출력층 사이를 여러 층으로 연결,  뉴런이 여러 개 연결되어 깊은 사고를 하는 것을 의미
  - 처리할 계산량이 기하급수적으로 늘어나 그 만큼 정확도가 높아진 결과를 얻을 수 있음
- 깊은 딥러닝 : 여러층의 은닉층의 단계가 많아 지는 것

![image-20220522155021444](https://user-images.githubusercontent.com/102509786/169683141-e3d21569-b719-4b32-baae-6317fc3cdb43.png)



### 손실 함수 (loss function)

- 비용 함수(cost function)
- 학습의 목표가 되는 기준 지표의 계산식, 목적함수(objective function)
  - 오류를 최소화하는 방향으로 학습을 수행
- 손실과 비용이라는 용어
  - 값이 작아질수록 목표에 접근



### 최적화 함수

- 손실 함수의 결과값을 최소화하는 함수
- 경사하강법(Gradient Descent)
- 확률적 경사하강법(SGD : Stochastic Gradient Descent)
- RMSProp

#### 경사 하강법(gradien descent)

- 말 그대로 경사 따라 내려가기 방식
- 기울기를 계산해 기울어진 방향으로 조금씩 이동하는 과정을 반복적으로 수행
- 기울기가 최소가 되는 지점으로 이동하는 방식
- 손실함수의 값이 최소가 되는 지점을 찾아 가는 방법

![image-20220522155400324](https://user-images.githubusercontent.com/102509786/169683142-1b3f4bba-1343-4bed-90f0-a67f7c77a997.png)

#### 가중치 계산

- 초기화된 가중치는 입력이 반복됨에 따라 더욱 적합한 값을 가질 수 있도록 첫 예측값 Y와 진짜 타깃 Y의 오차를 최소화
- 손실 함수(Loss Function)로 오차를 구함
- 최적화 함수(Optimization Function)를 사용하여 조절
- 활성화 함수(Activation Function)로 계산하여 다음 입력으로 사용

![image-20220522160307719](https://user-images.githubusercontent.com/102509786/169683143-bd3579e3-4323-4f3c-864c-d22f72e85c80.png)