# 7. 로지스틱 회귀

## 로지스틱 회귀란?

### 로지스틱 회귀의 개요

- 베르누이 분포 : 긍정 확률 P와 부정 확률 1-P를 갖는 랜덤 변수의 확률 분포 (동전 던지기 등)
- 베르누이 분포를 따르는 반응 변수의 확률 범위는 [0, 1]
- 선형 회귀는 특징 값의 일정한 변화로 반응 변수에도 일정한 변화가 일어난다는 가정을 하므로, 반응 변수가 확률을 나타낼 경우 가정이 유효하지 않음
- 일반화 선형 모델은 연결 함수를 사용해 특징의 선형 조합을 반응 변수와 연결 짓는 방법을 통해 선형 회귀의 가정을 없앰
- 특징의 선형 조합을 정규 분포를 따르지 않는 반응 변수와 연계하려면 연결함수가 필요



## 로지스틱 회귀의 특징

### 로지스틱 회귀와 분류

- 로지스틱 회귀의 반응 변수의 값 = 긍정(양성) 클래스의 확률
- 반응 변수의 값 >= 임계치 -> 긍정(양성) 클래스를 예측
- 반응 변수의 값 < 임계치 -> 부정(음성) 클래스를 예측
- 반응 변수는 '로지스틱 함수'를 사용해 특징의 선형 조합 함수로 모델링 됨

![image-20220512201439562](https://user-images.githubusercontent.com/102509786/168068083-bda51feb-cdb3-4497-b703-ef50d491d409.png)

- e는 오일러 상수(2.718...), t는 설명 변수의 조합 (β_0+βx)

β  : 가중치

β_0  : 상수항



### 로지스틱 함수 = 시그모이드 함수

```python
def sigmoid(z):
    return 1.0 / (1 + np.exp(-z))
```

```python
z = np.linspace(-10, 10, 1000)
y = sigmoid(z)
plt.plot(z, y)
plt.axhline(y=0, linestyle=":", color="black")
plt.axhline(y=0.5, linestyle=":", color='black')
plt.axhline(y=1, linestyle=":", color='black')
plt.yticks([0.0, 0.25, 0.5, 0.75, 1.0])
plt.xlabel('z')
plt.ylabel('y(z)')
```

![image-20220512202132931](https://user-images.githubusercontent.com/102509786/168068087-c5f005b7-cb27-469a-ba78-73af7ac41b93.png)

로지스틱 회귀는 이진분류 역할을 수행

(ex. 양성종양 vs 악성종양, 흡연 vs 비흡연)

- 로짓 함수는 로지스틱 함수의 역함수: F(x)를 다시 특징의 조합으로 돌림

역함수를 적용하면 선형 방정식을 얻을 수 있음

![image-20220512202221591](https://user-images.githubusercontent.com/102509786/168068088-1f89010f-9bd4-4581-8912-836658690f19.png)



## (1) 데이터 준비

```python
train_df = pd.read_csv('train.csv', nrows=100000)
unused_columns, label_column = ['id', 'hour' 'device_id', 'device_ip'], 'click'
train_df = train_df.drop(unused_columns, axis=1)
X_dict_train = list(train_df.drop(label_column, axis=1).T.do_dict().values())
y_train = train_df[label_column]
```

```python
test_df = pd.read_csv('train.csv', header=0, skiprows=(1, 100000), nrows=100000)
test_df = test_df.drop(unused_columns, axis=1)
X_dict_test = list(test_df.drop(label_column, axis=1).T.to_dict().values())
y_test = test_df[label_column]
```

## (2) 원 핫 인코딩 벡터 변환

```python
from sklearn.feature_extraction import DictVectorizer

vectorizer = DictVectorizer(sparse=True)
X_train = vectorizer.fit_transform(X_dict_train)
X_test = vectorizer.transform(X_dict_test)
```

## (3) 그리드 서치를 이용한 로지스틱 회귀 모델 실습

```python
from sklearn.linear_model.logistic import LogisticRegression #이진분류, 대규모 데이터를 다룰 때 유용

clf = LogisticRegression()
clf.fit(X_train, y_train)
```

![image-20220512203218870](https://user-images.githubusercontent.com/102509786/168068089-cab92eb3-47b4-436b-a4fd-c923254544ed.png)

C : L1, L2 규제강도 (0에 가까울 수록 규제가 강하고 값이 커질수록 규제가 느슨)

Penalty : L1, L2 규제 적용여부

```python
from sklearn.model_selection import GridSearchCV

parameters = {'C': {0.001, 0.01, 0.1, 1. 10}, 'penalty':['l1', 'l2']}
grid_search = GridSearchCV(clf, parameters, n_jobs=-1, cv=3, scoring='roc_auc')
grid_search.fit(X_train, y_train)
```

![image-20220512203445619](https://user-images.githubusercontent.com/102509786/168068092-b2fc07db-ebf8-43d7-a8ee-22b64c95eb25.png)

```python
grid_search.best_params_
```

{'C': 1, 'penalty': 'l1'}

## (4) 로지스틱 회귀 모델의 성능 측정 : 정확도, 혼동행렬, ROC의 AUC

```python
clf_best = grid_search.best_estimator_

y_pred = clf_pest.predict(X_test)
y_pred
# array([0, 0, 0, ..., 0, 0, 0], dtype=int64)
```

```python
np.unique(y_pred, return_counts=True)
# (array([0, 1], dtype=int64), array([95963, 4037], dtype=int64))
```

```python
from sklearn.metrics import accuracy_score
accuracy_score(y_test, y_pred)
# 0.83203
```

```python
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, y_pred)
# array([[80838, 1672],
# 		 [15125, 2365]], dtype=int64)
```

```python
from sklearn.metrics import roc_auc_score, roc_curve
y_pred_proba = clf_best.proba(X_test)[:, 1]
y_pred_proba
# array([0.2041158, 0.12872102, 0.2872037, ..., 0.13072682, 0.09605461, 0.05706108])
```

**로지스틱 회귀의 경우 predict() 대신 predict_proba() 함수를 사용**

```python
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
auc = roc_auc_score(y_test, y_pred_proba)
plt.plot(fpr, tpr, 'r-', label='LogisticRegression')
plt.plot([0, 1], [0, 1], 'b--', label='random guess')
plt.xlabel('false positive rate')
plt.ylabel('true positive rate')
plt.title('AUC= {0:.2f}'.format(auc))
plt.legend(loc='lower right')
```

![image-20220512204519327](https://user-images.githubusercontent.com/102509786/168068093-bb4dd44c-bc3d-4be1-8339-50713b93f4c5.png)

## 주요 정리

1. 로지스틱 회귀의 반응 변수는 긍정(양성) 클래스의 확률 값을 가진다.
2. 로지스틱 회귀의 반응 변수의 값이 임계치 이상이면 긍정(양성) 클래스를 예측하고, 반응 변수의 값이 임계치 미만이면 부정(음성) 클래스를 예측한다.
3. 반응 변수는 ‘로지스틱 함수(시그모이드 함수)’를 사용해 특징의 선형 조합 함수로 모델링된다.