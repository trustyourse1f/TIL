# 8. knn

## K-최근접 이웃과 분류

### k-최근접 이웃의 개요

- 분류나 회귀에 사용할 수 있는 알고리즘으로 단순해 보이지만 강력하고 유용한 기법
- 비매개변수 머신러닝 모델
- 훈련 단계에서 학습을 하지 않기 때문에 '게으른 학습'이라 부름
- 테스트 / 검증 단계에서 테스트 관측값과 가장 근접한 훈련 관측값을 비교
- 거리에만 의존하므로 차원의 저주에 따라 예측에 필요한 특징의 개수가 늘어나면 성능이 크게 저하됨



### k-최근접 이웃의 특징

- 유클리드 공간의 점과 점 사이의 직선거리를 사용
- 2차원 공간의 유클리드 거리의 계산방법

예![image-20220513134406623](https://user-images.githubusercontent.com/102509786/168273891-cb3485c8-c234-4081-90b9-27235698ea33.png)

- k개의 가장 가까운 훈련 인스턴스를 골라 가장 많은 레이블을 분류로 선택
- 특징이 표준화된 크기 조절(스케일링)이 필요

표준점수(Z-score)를 활용할 필요 있음



### 차원의 저주 : 1차원

```python
df_1d = pd.DataFrame(data=np.random.rand(60, 1), columns=['ld_points'])
df_1d['height'] = 1
plt.scatter(df_1d['ld_points'], df_1d['height'])
plt.yticks([])
plt.xlabel(df_1d.columns[0])
```

![image-20220513134748979](https://user-images.githubusercontent.com/102509786/168273637-247cc9d4-3855-42b7-aa0c-9c93ef8b9fa3.png)



### 차원의 저주 : 2차원

```python
df_2d = pd.DataFrame(data=np.random.rand(60, 2), columns=['x', 'y'])
plt.scatter(df_2d['x'], df_2d['y'])
plt.yticks([])
plt.xlabel(df_2d.columns[0])
plt.ylabel(df_2d.columns[1])
```

![image-20220513135030297](https://user-images.githubusercontent.com/102509786/168273638-a107c60c-44fc-4d98-b125-dfc459a88208.png)



### 차원의 저주 : 3차원

```python
df_3d = pd.DataFrame(data=np.random.rand(60, 3), columns=['x', 'y', 'z'])

from mpl_toolkits.mplot3d import Axes3D
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(df_3d['x'], df_3d['y'], df_3d['y'])
```



![image-20220513141502655](https://user-images.githubusercontent.com/102509786/168273639-b449b3ac-9b93-47f8-a810-587395545ab8.png)

차원이 늘어날수록 점과 점 사이 거리가 멀어져 분류기 작동이 어려워짐

### 데이터 프레임의 생성

```python
breast_cancer = pd.read_csb('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data', header=None)
breast_cancer.head()
```

![image-20220513195347227](https://user-images.githubusercontent.com/102509786/168273645-075ef791-07e2-46da-9b15-e4499a7bbde5.png)

컬럼명이 없으므로 새로 만들어야 함

### 데이터 프레임의 컬럼 정보 갱신

```python
breast_cancer.columns = ['id_number', 'clump_thickness', 'unif_cell_size', 'unif_cell_shape', 'marg_adhesion', 'single_epith_cell_size', 'bare_nuclei', 'bland_chromatin', 'normal_nucleoli', 'mitoses', 'class']
breast_cancer.head()
```

![image-20220513195720107](https://user-images.githubusercontent.com/102509786/168273646-cac2b3ae-406d-4208-bc3a-d5d28577f836.png)

### 누락 값의 대체 및 클래스 레이블을  0과 1로 변환

```python
breast_cancer.info()
```

![image-20220513195841323](https://user-images.githubusercontent.com/102509786/168273649-1450f8d5-eb62-4e7d-9535-37240fe32e08.png)

object 타입 → 문자열로된 값이 있을 수 있음

```python
breat_cancer.isnull().values.sum()
# 데이터프레임 전체에서 null값의 개수

```

```python
breast_cancer['bare_nuclei'] = breast_cancer['bare_nuclei'].replace('?', np.NAN) # ‘?’ 값을 null로 교체
breast_cancer['bare_nuclei'] = breast_cancer['bare_nuclei'].fillna(breast_cancer['bare_nuclei'].value_counts().index[0])
# Null값을 최빈값으로 대체

```

```python
breast_cancer['cancer_ind'] = 0
breast_cancer.loc[breast_cancer['class']==4, 'cancer_ind'] = 1
#새로운 컬럼 ‘cancer_ind’을 생성하고 악성종양(4)인 데이터를 1로 설정

```

### 불필요한 변수제거 및 표준화 적용

```python
X = breast_cancer.drop(['id_number', 'class', 'cancer_ind'], axis=1)
y = breast_cancer.cancer_ind

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(0.3), random_state=42)

from sklearn.preprocessing import StandardScaler  # 데이터 표준화 객체 생성
scaler = StandardScaler()

X_train_scaled = scaler.fit_transfrom(X_train)
X_test_scaled = scaler.transform(X_test)
# 훈련 데이터, 테스트 데이터를 표준화

```

### 머신러닝 모델 클래스 KNeighborsClassifier를 이용한 학습

```python
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train_scaled, y_train)
```

### 분류 모델의 혼동행렬, 정확도, AUC

```python
from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve

y_pred = knn.predict(X_test_scaled)
```

```python
accuracy_score(y_test, y_pred)
```

0.9761904761904762

```python
confusion_matrix(y_test, y_pred)
```

array([[141, 2],

​			[3, 64]], dtype=int54)

```python
roc_auc_score(y_test, y_pred)
```

0.9706189333055005

### 그리드 서치를 이용한 하이퍼파라미터의 최적 값 선택

```python
from sklearn.model_selection import GridSearchCV

grid_search = GridSearchCV(knn, {'n_neighbors': [1, 2, 3, 4, 5]}. n_jobs=-1, cv=7, scoring='roc_auc')

grid_search.fit(X_train, y_train)
```

```python
grid_search.best_params_
```

{'n_neighbors': 5}

```python
knn_best = grid_search.best_estimator_

y_pred = knn_best.predict(X_test_scaled)
```

```python
accuracy_score(y_test, y_pred)
```

0.9666666666666667

```python
confusion_matrix(y_test, y_pred)
```

array([[141, 2],

​			[5, 62]], dtype=int64)

```python
roc_auc_score(y_test, y_pred)
```

0.9556935601711722



## 주요 정리

1. K-최근접 이웃은 비매개변수 머신러닝 모델로 훈련 단계에서 학습을 하지 않기 때문에  ‘게으른 학습’이라 불리우며, 테스트/검증 단계에서 테스트 관측값과 가장 근접한 훈련 관측값을 비교 작업을 수행한다.
2. K-최근접 이웃은 거리에만 의존하므로 차원의 저주에 의해 예측에 필요한 특징의 개수가 늘어나면 성능이 크게 저하된다.
3. K-최근접 이웃 분류기의 성능측정은 혼동행렬을 이용해 정확도, AUC와 같은 다양한 평가지표를 활용할 수 있다.
4. 그리드 서치를 이용해 n_neighbor와 같은 k-최근접 이웃 하이퍼파라미터의 최적 값을 찾아낼 수 있다.